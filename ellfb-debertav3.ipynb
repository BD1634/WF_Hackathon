{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abd1b76f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:17:46.663362Z",
     "iopub.status.busy": "2022-11-22T04:17:46.662824Z",
     "iopub.status.idle": "2022-11-22T04:17:53.801801Z",
     "shell.execute_reply": "2022-11-22T04:17:53.800766Z"
    },
    "id": "85ae051a",
    "outputId": "7417e36b-88ee-4670-f3e1-b8c07171ca96",
    "papermill": {
     "duration": 7.153244,
     "end_time": "2022-11-22T04:17:53.804439",
     "exception": false,
     "start_time": "2022-11-22T04:17:46.651195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers.__version__: 4.20.1\n",
      "tokenizers.__version__: 0.12.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "from distutils.util import strtobool\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import datetime\n",
    "import transformers\n",
    "import tokenizers\n",
    "print(f'transformers.__version__: {transformers.__version__}')\n",
    "print(f'tokenizers.__version__: {tokenizers.__version__}')\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "os.environ['TOKENIZERS_PARALLELISM']='true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fff7682",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:17:53.823179Z",
     "iopub.status.busy": "2022-11-22T04:17:53.822685Z",
     "iopub.status.idle": "2022-11-22T04:17:53.850902Z",
     "shell.execute_reply": "2022-11-22T04:17:53.849835Z"
    },
    "papermill": {
     "duration": 0.040621,
     "end_time": "2022-11-22T04:17:53.853886",
     "exception": false,
     "start_time": "2022-11-22T04:17:53.813265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/ellfb-debertav3/20221121-172724-deberta-v3-base.csv\n",
      "/kaggle/input/ellfb-debertav3/__results__.html\n",
      "/kaggle/input/ellfb-debertav3/__notebook__.ipynb\n",
      "/kaggle/input/ellfb-debertav3/__output__.json\n",
      "/kaggle/input/ellfb-debertav3/custom.css\n",
      "/kaggle/input/ellfb-debertav3/20221121-172724-deberta-v3-base/microsoft-deberta-v3-base_fold1_best.pth\n",
      "/kaggle/input/ellfb-debertav3/20221121-172724-deberta-v3-base/train.log\n",
      "/kaggle/input/ellfb-debertav3/20221121-172724-deberta-v3-base/microsoft-deberta-v3-base_fold2_best.pth\n",
      "/kaggle/input/ellfb-debertav3/20221121-172724-deberta-v3-base/microsoft-deberta-v3-base_fold0_best.pth\n",
      "/kaggle/input/ellfb-debertav3/20221121-172724-deberta-v3-base/oof_df.pkl\n",
      "/kaggle/input/ellfb-debertav3/20221121-172724-deberta-v3-base/model/config.json\n",
      "/kaggle/input/ellfb-debertav3/20221121-172724-deberta-v3-base/model/pytorch_model.bin\n",
      "/kaggle/input/ellfb-debertav3/20221121-172724-deberta-v3-base/tokenizer/spm.model\n",
      "/kaggle/input/ellfb-debertav3/20221121-172724-deberta-v3-base/tokenizer/tokenizer.json\n",
      "/kaggle/input/ellfb-debertav3/20221121-172724-deberta-v3-base/tokenizer/tokenizer_config.json\n",
      "/kaggle/input/ellfb-debertav3/20221121-172724-deberta-v3-base/tokenizer/special_tokens_map.json\n",
      "/kaggle/input/ellfb-debertav3/20221121-172724-deberta-v3-base/tokenizer/added_tokens.json\n",
      "/kaggle/input/ellfb-debertav3/20221121-172724-deberta-v3-base/config/config.json\n",
      "/kaggle/input/feedback-prize-english-language-learning/sample_submission.csv\n",
      "/kaggle/input/feedback-prize-english-language-learning/train.csv\n",
      "/kaggle/input/feedback-prize-english-language-learning/test.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbca4fe",
   "metadata": {
    "id": "bd8d61dc",
    "papermill": {
     "duration": 0.008436,
     "end_time": "2022-11-22T04:17:53.870648",
     "exception": false,
     "start_time": "2022-11-22T04:17:53.862212",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed19c55a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:17:53.890381Z",
     "iopub.status.busy": "2022-11-22T04:17:53.890105Z",
     "iopub.status.idle": "2022-11-22T04:17:53.899570Z",
     "shell.execute_reply": "2022-11-22T04:17:53.898602Z"
    },
    "id": "a5bdb746",
    "papermill": {
     "duration": 0.021512,
     "end_time": "2022-11-22T04:17:53.902004",
     "exception": false,
     "start_time": "2022-11-22T04:17:53.880492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    str_now = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "    train = False\n",
    "    test = True\n",
    "    debug = False\n",
    "    offline = False\n",
    "    models_path = 'feedback-models'\n",
    "    epochs = 3\n",
    "    save_all_models = False\n",
    "    competition = 'Feedback'\n",
    "    apex = True\n",
    "    print_freq = 20\n",
    "    num_workers = 4\n",
    "    model = 'microsoft/deberta-v3-base' #If you want to train on the kaggle platform, v3-base is realistic. v3-large will time out.\n",
    "    loss_func = 'RMSE' # 'SmoothL1', 'RMSE'\n",
    "    gradient_checkpointing = True\n",
    "    scheduler = 'cosine'\n",
    "    batch_scheduler = True\n",
    "    num_cycles = 0.5\n",
    "    num_warmup_steps = 0\n",
    "    encoder_lr = 2e-5\n",
    "    decoder_lr = 2e-5\n",
    "    min_lr = 1e-6\n",
    "    #Layer-Wise Learning Rate Decay\n",
    "    llrd = True\n",
    "    layerwise_lr = 5e-5\n",
    "    layerwise_lr_decay = 0.9\n",
    "    layerwise_weight_decay = 0.01\n",
    "    layerwise_adam_epsilon = 1e-6\n",
    "    layerwise_use_bertadam = False\n",
    "    #pooling\n",
    "    pooling = 'attention' # mean, max, min, attention, weightedlayer\n",
    "    layer_start = 4\n",
    "    #init_weight\n",
    "    init_weight = 'normal' # normal, xavier_uniform, xavier_normal, kaiming_uniform, kaiming_normal, orthogonal\n",
    "    #re-init\n",
    "    reinit = True\n",
    "    reinit_n = 1\n",
    "    #adversarial\n",
    "    fgm = True\n",
    "    awp = False\n",
    "    adv_lr = 1\n",
    "    adv_eps = 0.2\n",
    "    unscale = False\n",
    "    eps = 1e-6\n",
    "    betas = (0.9, 0.999)\n",
    "    max_len = 400\n",
    "    weight_decay = 0.01\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm = 1000\n",
    "    target_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    seed = 42\n",
    "    cv_seed = 42\n",
    "    n_fold = 3\n",
    "    trn_fold = list(range(n_fold))\n",
    "    batch_size = 8\n",
    "    n_targets = 6\n",
    "    gpu_id = 0\n",
    "    device = f'cuda:{gpu_id}'\n",
    "    train_file = '/kaggle/input/feedback-prize-english-language-learning/train.csv'\n",
    "    test_file = '/kaggle/input/feedback-prize-english-language-learning/test.csv'\n",
    "    submission_file = '/kaggle/input/feedback-prize-english-language-learning/sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeca7505",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:17:53.920186Z",
     "iopub.status.busy": "2022-11-22T04:17:53.919916Z",
     "iopub.status.idle": "2022-11-22T04:17:53.926596Z",
     "shell.execute_reply": "2022-11-22T04:17:53.925465Z"
    },
    "id": "35118197",
    "outputId": "d6badf71-a8a3-4fd8-dc16-959657bfbfb1",
    "papermill": {
     "duration": 0.018069,
     "end_time": "2022-11-22T04:17:53.928565",
     "exception": false,
     "start_time": "2022-11-22T04:17:53.910496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/ellfb-debertav3/20221121-172724-deberta-v3-base/\n"
     ]
    }
   ],
   "source": [
    "#Unique model name\n",
    "if CFG.train:\n",
    "    if len(CFG.model.split(\"/\")) == 2:\n",
    "        CFG.identifier = f'{CFG.str_now}-{CFG.model.split(\"/\")[1]}'\n",
    "    else:\n",
    "        CFG.identifier = f'{CFG.str_now}-{CFG.model}'\n",
    "else:\n",
    "    CFG.identifier = \"/kaggle/input/ellfb-debertav3/20221121-172724-deberta-v3-base/\"\n",
    "    CFG.OUTPUT_DIR = f'./20221121-172724-deberta-v3-base/'\n",
    "    CFG.log_filename = CFG.OUTPUT_DIR + 'test'\n",
    "    os.makedirs(CFG.OUTPUT_DIR, exist_ok=True)\n",
    "print(CFG.identifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec898106",
   "metadata": {
    "id": "e5e7ca6a",
    "papermill": {
     "duration": 0.008365,
     "end_time": "2022-11-22T04:17:53.945448",
     "exception": false,
     "start_time": "2022-11-22T04:17:53.937083",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Read train and split with MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5f1939d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:17:53.964712Z",
     "iopub.status.busy": "2022-11-22T04:17:53.964013Z",
     "iopub.status.idle": "2022-11-22T04:17:53.973252Z",
     "shell.execute_reply": "2022-11-22T04:17:53.972381Z"
    },
    "id": "b10683c9",
    "outputId": "81a12101-8e5b-42e9-e06f-9512f41d5a96",
    "papermill": {
     "duration": 0.021016,
     "end_time": "2022-11-22T04:17:53.975235",
     "exception": false,
     "start_time": "2022-11-22T04:17:53.954219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.train:\n",
    "    CFG.df_train = pd.read_csv(CFG.train_file)\n",
    "    CFG.OUTPUT_DIR = f'./{CFG.identifier}/'\n",
    "    CFG.log_filename = CFG.OUTPUT_DIR + 'train'\n",
    "    if CFG.offline:\n",
    "        #TO DO\n",
    "        pass\n",
    "    else:\n",
    "        os.system('pip install iterative-stratification==0.1.7')\n",
    "    #CV\n",
    "#     x_train , x_val = train_test_split(CFG.df_train,random_state=42,test_size=0.3)\n",
    "    \n",
    "\n",
    "    from iterstrat.ml_stratifiers import MultilabelStratifiedKFold    \n",
    "    Fold = MultilabelStratifiedKFold(n_splits = CFG.n_fold, shuffle = True, random_state = CFG.cv_seed)\n",
    "    for n, (train_index, val_index) in enumerate(Fold.split(CFG.df_train, CFG.df_train[CFG.target_cols])):\n",
    "        CFG.df_train.loc[val_index, 'fold'] = int(n)\n",
    "    import re\n",
    "    def clean_text(text):\n",
    "        text = re.sub(r\"(\\n|\\r)+\",\" \",text.strip())\n",
    "        text = re.sub(r\"\\s+\",\" \",text)\n",
    "        return text\n",
    "    CFG.df_train['text'] = CFG.df_train['full_text'].apply(clean_text)\n",
    "    CFG.df_train['fold'] = CFG.df_train['fold'].astype(int)\n",
    "    os.makedirs(CFG.OUTPUT_DIR, exist_ok=True)    \n",
    "    print(CFG.OUTPUT_DIR)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.trn_fold = [0]\n",
    "    if CFG.train:\n",
    "        CFG.df_train = CFG.df_train.sample(n = 100, random_state = CFG.seed).reset_index(drop=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a0c425",
   "metadata": {
    "id": "364e9c27",
    "papermill": {
     "duration": 0.008093,
     "end_time": "2022-11-22T04:17:53.992281",
     "exception": false,
     "start_time": "2022-11-22T04:17:53.984188",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96ad9db2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:17:54.010441Z",
     "iopub.status.busy": "2022-11-22T04:17:54.010166Z",
     "iopub.status.idle": "2022-11-22T04:17:54.029852Z",
     "shell.execute_reply": "2022-11-22T04:17:54.029040Z"
    },
    "id": "9d6dcaba",
    "papermill": {
     "duration": 0.03109,
     "end_time": "2022-11-22T04:17:54.031866",
     "exception": false,
     "start_time": "2022-11-22T04:17:54.000776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def MCRMSE(y_trues, y_preds):\n",
    "    scores = []\n",
    "    idxes = y_trues.shape[1]\n",
    "    for i in range(idxes):\n",
    "        y_true = y_trues[:, i]\n",
    "        y_pred = y_preds[:, i]\n",
    "        score = mean_squared_error(y_true, y_pred, squared = False)\n",
    "        scores.append(score)\n",
    "    mcrmse_score = np.mean(scores)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "def get_score(y_trues, y_preds):\n",
    "    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "def get_logger(filename = CFG.log_filename):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter('%(message)s'))\n",
    "    handler2 = FileHandler(filename = f'{filename}.log')\n",
    "    handler2.setFormatter(Formatter('%(message)s'))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "def prepare_input(cfg, text):\n",
    "    inputs = cfg.tokenizer.encode_plus(\n",
    "        text,\n",
    "        return_tensors = None,\n",
    "        add_special_tokens = True,\n",
    "        max_length = cfg.max_len,\n",
    "        pad_to_max_length = True,\n",
    "        truncation = True\n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype = torch.long)\n",
    "    return inputs    \n",
    "\n",
    "def collate(inputs):\n",
    "    mask_len = int(inputs['attention_mask'].sum(axis = 1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:, :mask_len]\n",
    "    return inputs\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def update(self, val, n = 1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return f'{int(m)}m {int(s)}s'\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return f'{str(asMinutes(s))} (remain {str(asMinutes(rs))})'\n",
    "\n",
    "def seed_everything(seed = 42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, reduction = 'mean', eps = 1e-9):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss(reduction = 'none')\n",
    "        self.reduction = reduction\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n",
    "        if self.reduction == 'none':\n",
    "            loss = loss\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b7998a",
   "metadata": {
    "id": "55633022",
    "papermill": {
     "duration": 0.008327,
     "end_time": "2022-11-22T04:17:54.048977",
     "exception": false,
     "start_time": "2022-11-22T04:17:54.040650",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pooling\n",
    "\n",
    "* Attention pooling (https://www.kaggle.com/competitions/feedback-prize-english-language-learning/discussion/361678)\n",
    "* WeightedLayerPooling (https://www.kaggle.com/code/rhtsingh/on-stability-of-few-sample-transformer-fine-tuning?scriptVersionId=67176591&cellId=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2731347",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:17:54.067577Z",
     "iopub.status.busy": "2022-11-22T04:17:54.067310Z",
     "iopub.status.idle": "2022-11-22T04:17:54.082525Z",
     "shell.execute_reply": "2022-11-22T04:17:54.081413Z"
    },
    "id": "ed191bd8",
    "papermill": {
     "duration": 0.026894,
     "end_time": "2022-11-22T04:17:54.084400",
     "exception": false,
     "start_time": "2022-11-22T04:17:54.057506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min = 1e-9)\n",
    "        mean_embeddings = sum_embeddings/sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "class MaxPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaxPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        embeddings = last_hidden_state.clone()\n",
    "        embeddings[input_mask_expanded == 0] = -1e4\n",
    "        max_embeddings, _ = torch.max(embeddings, dim = 1)\n",
    "        return max_embeddings\n",
    "    \n",
    "class MinPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MinPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        embeddings = last_hidden_state.clone()\n",
    "        embeddings[input_mask_expanded == 0] = 1e-4\n",
    "        min_embeddings, _ = torch.min(embeddings, dim = 1)\n",
    "        return min_embeddings\n",
    "\n",
    "#Attention pooling\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "        nn.Linear(in_dim, in_dim),\n",
    "        nn.LayerNorm(in_dim),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(in_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        w = self.attention(last_hidden_state).float()\n",
    "        w[attention_mask==0]=float('-inf')\n",
    "        w = torch.softmax(w,1)\n",
    "        attention_embeddings = torch.sum(w * last_hidden_state, dim=1)\n",
    "        return attention_embeddings\n",
    "\n",
    "#There may be a bug in my implementation because it does not work well.\n",
    "class WeightedLayerPooling(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n",
    "        super(WeightedLayerPooling, self).__init__()\n",
    "        self.layer_start = layer_start\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layer_weights = layer_weights if layer_weights is not None \\\n",
    "            else nn.Parameter(\n",
    "                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n",
    "            )\n",
    "\n",
    "    def forward(self, ft_all_layers):\n",
    "        all_layer_embedding = torch.stack(ft_all_layers)\n",
    "        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n",
    "\n",
    "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
    "        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
    "\n",
    "        return weighted_average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7e0092",
   "metadata": {
    "id": "92707e50",
    "papermill": {
     "duration": 0.008675,
     "end_time": "2022-11-22T04:17:54.101635",
     "exception": false,
     "start_time": "2022-11-22T04:17:54.092960",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fast Gradient Method (FGM)\n",
    "Reference :\n",
    "\n",
    "https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/143764"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7bc0988",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:17:54.120222Z",
     "iopub.status.busy": "2022-11-22T04:17:54.119467Z",
     "iopub.status.idle": "2022-11-22T04:17:54.127162Z",
     "shell.execute_reply": "2022-11-22T04:17:54.126302Z"
    },
    "id": "857cb5c4",
    "papermill": {
     "duration": 0.01882,
     "end_time": "2022-11-22T04:17:54.129168",
     "exception": false,
     "start_time": "2022-11-22T04:17:54.110348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FGM():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self, epsilon = 1., emb_name = 'word_embeddings'):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0:\n",
    "                    r_at = epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "\n",
    "    def restore(self, emb_name = 'word_embeddings'):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "            self.backup = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e175849c",
   "metadata": {
    "id": "466b0768",
    "papermill": {
     "duration": 0.008311,
     "end_time": "2022-11-22T04:17:54.146194",
     "exception": false,
     "start_time": "2022-11-22T04:17:54.137883",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Adversarial Weight Perturbation (AWP)\n",
    "There may be a bug in my implementation because it does not work well.\n",
    "\n",
    "Reference : \n",
    "\n",
    "https://www.kaggle.com/code/wht1996/feedback-nn-train/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23eee1e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:17:54.164111Z",
     "iopub.status.busy": "2022-11-22T04:17:54.163825Z",
     "iopub.status.idle": "2022-11-22T04:17:54.178633Z",
     "shell.execute_reply": "2022-11-22T04:17:54.177763Z"
    },
    "id": "21375aec",
    "papermill": {
     "duration": 0.026022,
     "end_time": "2022-11-22T04:17:54.180741",
     "exception": false,
     "start_time": "2022-11-22T04:17:54.154719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AWP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        optimizer,\n",
    "        adv_param=\"weight\",\n",
    "        adv_lr=1,\n",
    "        adv_eps=0.2,\n",
    "        start_epoch=0,\n",
    "        adv_step=1,\n",
    "        scaler=None\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.adv_param = adv_param\n",
    "        self.adv_lr = adv_lr\n",
    "        self.adv_eps = adv_eps\n",
    "        self.start_epoch = start_epoch\n",
    "        self.adv_step = adv_step\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}\n",
    "        self.scaler = scaler\n",
    "        \n",
    "    def attack_backward(self, x, y, attention_mask,epoch):\n",
    "        if (self.adv_lr == 0) or (epoch < self.start_epoch):\n",
    "            return None\n",
    "\n",
    "        self._save() \n",
    "        for i in range(self.adv_step):\n",
    "            self._attack_step() \n",
    "            with torch.cuda.amp.autocast():\n",
    "                adv_loss, tr_logits = self.model(input_ids=x, attention_mask=attention_mask, labels=y)\n",
    "                adv_loss = adv_loss.mean()\n",
    "            self.optimizer.zero_grad()\n",
    "            self.scaler.scale(adv_loss).backward()\n",
    "            \n",
    "        self._restore()\n",
    "        \n",
    "    def _attack_step(self):\n",
    "        e = 1e-6\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                norm1 = torch.norm(param.grad)\n",
    "                norm2 = torch.norm(param.data.detach())\n",
    "                if norm1 != 0 and not torch.isnan(norm1):\n",
    "                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n",
    "                    param.data.add_(r_at)\n",
    "                    param.data = torch.min(\n",
    "                        torch.max(param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n",
    "                    )\n",
    "                # param.data.clamp_(*self.backup_eps[name])\n",
    "\n",
    "    def _save(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                if name not in self.backup:\n",
    "                    self.backup[name] = param.data.clone()\n",
    "                    grad_eps = self.adv_eps * param.abs().detach()\n",
    "                    self.backup_eps[name] = (\n",
    "                        self.backup[name] - grad_eps,\n",
    "                        self.backup[name] + grad_eps,\n",
    "                    )\n",
    "\n",
    "    def _restore(self,):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in self.backup:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b45969d",
   "metadata": {
    "id": "b6bdf2ac",
    "papermill": {
     "duration": 0.008361,
     "end_time": "2022-11-22T04:17:54.197617",
     "exception": false,
     "start_time": "2022-11-22T04:17:54.189256",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train function\n",
    "* FGM\n",
    "* AWP (There may be a bug.)\n",
    "* Unscale optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f60091bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:17:54.217399Z",
     "iopub.status.busy": "2022-11-22T04:17:54.217079Z",
     "iopub.status.idle": "2022-11-22T04:17:54.231071Z",
     "shell.execute_reply": "2022-11-22T04:17:54.230106Z"
    },
    "id": "7a53dbdb",
    "papermill": {
     "duration": 0.026321,
     "end_time": "2022-11-22T04:17:54.233090",
     "exception": false,
     "start_time": "2022-11-22T04:17:54.206769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    losses = AverageMeter()\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled = CFG.apex)\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    if CFG.fgm:\n",
    "        fgm = FGM(model)\n",
    "    if CFG.awp:\n",
    "        awp = AWP(model,\n",
    "                  optimizer, \n",
    "                  adv_lr = CFG.adv_lr, \n",
    "                  adv_eps = CFG.adv_eps, \n",
    "                  scaler = scaler)\n",
    "    for step, (inputs, labels) in enumerate(train_loader):\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled = CFG.apex):\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds, labels)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        if CFG.unscale:\n",
    "            scaler.unscale_(optimizer)\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        \n",
    "        #Fast Gradient Method (FGM)\n",
    "        if CFG.fgm:\n",
    "            fgm.attack()\n",
    "            with torch.cuda.amp.autocast(enabled = CFG.apex):\n",
    "                y_preds = model(inputs)\n",
    "                loss_adv = criterion(y_preds, labels)\n",
    "                loss_adv.backward()\n",
    "            fgm.restore()\n",
    "            \n",
    "        #Adversarial Weight Perturbation (AWP)\n",
    "        if CFG.awp:\n",
    "            loss_awp = awp.attack_backward(inputs, labels, attention_mask, step + 1)\n",
    "            loss_awp.backward()\n",
    "            awp._restore()\n",
    "        \n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f} '\n",
    "                  'LR: {lr:.8f} '\n",
    "                  .format(epoch + 1, step, len(train_loader), remain = timeSince(start, float(step + 1)/len(train_loader)),\n",
    "                          loss = losses,\n",
    "                          grad_norm = grad_norm,\n",
    "                          lr = scheduler.get_lr()[0]\n",
    "                         )\n",
    "                 )\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307097ec",
   "metadata": {
    "id": "97cdf99c",
    "papermill": {
     "duration": 0.008558,
     "end_time": "2022-11-22T04:17:54.250612",
     "exception": false,
     "start_time": "2022-11-22T04:17:54.242054",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Valid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "232277c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:17:54.271048Z",
     "iopub.status.busy": "2022-11-22T04:17:54.269456Z",
     "iopub.status.idle": "2022-11-22T04:17:54.279806Z",
     "shell.execute_reply": "2022-11-22T04:17:54.278815Z"
    },
    "id": "c593bba1",
    "papermill": {
     "duration": 0.022634,
     "end_time": "2022-11-22T04:17:54.281994",
     "exception": false,
     "start_time": "2022-11-22T04:17:54.259360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, labels) in enumerate(valid_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds, labels)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader) - 1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss = losses,\n",
    "                          remain = timeSince(start, float(step + 1) / len(valid_loader))\n",
    "                         )\n",
    "                 )\n",
    "    return losses.avg, np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f191847",
   "metadata": {
    "id": "59104d7e",
    "papermill": {
     "duration": 0.008894,
     "end_time": "2022-11-22T04:17:54.299810",
     "exception": false,
     "start_time": "2022-11-22T04:17:54.290916",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "124a647e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:17:54.319828Z",
     "iopub.status.busy": "2022-11-22T04:17:54.318887Z",
     "iopub.status.idle": "2022-11-22T04:17:54.326540Z",
     "shell.execute_reply": "2022-11-22T04:17:54.325241Z"
    },
    "id": "672fd430",
    "outputId": "d253b3c9-8241-4038-af78-6673a89c6217",
    "papermill": {
     "duration": 0.020026,
     "end_time": "2022-11-22T04:17:54.328722",
     "exception": false,
     "start_time": "2022-11-22T04:17:54.308696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.train:\n",
    "    LOGGER = get_logger()\n",
    "    LOGGER.info(f'OUTPUT_DIR: {CFG.OUTPUT_DIR}')\n",
    "    CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "    CFG.tokenizer.save_pretrained(CFG.OUTPUT_DIR + 'tokenizer')\n",
    "\n",
    "    #max_len\n",
    "    lengths = []\n",
    "    tk0 = tqdm(CFG.df_train['full_text'].fillna('').values, total = len(CFG.df_train))\n",
    "    for text in tk0:\n",
    "        length = len(CFG.tokenizer(text, add_special_tokens = False)['input_ids'])\n",
    "        lengths.append(length)\n",
    "    CFG.max_len = max(lengths) + 2\n",
    "    LOGGER.info(f'max_len: {CFG.max_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0767b95",
   "metadata": {
    "id": "cdeaf267",
    "papermill": {
     "duration": 0.008496,
     "end_time": "2022-11-22T04:17:54.345802",
     "exception": false,
     "start_time": "2022-11-22T04:17:54.337306",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "530755bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:17:54.365234Z",
     "iopub.status.busy": "2022-11-22T04:17:54.364373Z",
     "iopub.status.idle": "2022-11-22T04:17:54.370891Z",
     "shell.execute_reply": "2022-11-22T04:17:54.370064Z"
    },
    "id": "3ff61a44",
    "papermill": {
     "duration": 0.018288,
     "end_time": "2022-11-22T04:17:54.372938",
     "exception": false,
     "start_time": "2022-11-22T04:17:54.354650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FB3TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['text'].values\n",
    "        self.labels = df[cfg.target_cols].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg, self.texts[item])\n",
    "        label = torch.tensor(self.labels[item], dtype = torch.float)\n",
    "        return inputs, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a24374",
   "metadata": {
    "id": "a78dbf81",
    "papermill": {
     "duration": 0.008216,
     "end_time": "2022-11-22T04:17:54.390269",
     "exception": false,
     "start_time": "2022-11-22T04:17:54.382053",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model\n",
    "\n",
    "* Initializing module (normal, xavier_uniform, xavier_normal, kaiming_uniform, kaiming_normal, orthogonal) \n",
    "* Freeze lower layer when you use large model (v2-xlarge, funnnel, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68958b4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:17:54.410251Z",
     "iopub.status.busy": "2022-11-22T04:17:54.409945Z",
     "iopub.status.idle": "2022-11-22T04:17:54.435621Z",
     "shell.execute_reply": "2022-11-22T04:17:54.434652Z"
    },
    "id": "0f57c98c",
    "papermill": {
     "duration": 0.038669,
     "end_time": "2022-11-22T04:17:54.437888",
     "exception": false,
     "start_time": "2022-11-22T04:17:54.399219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FB3Model(nn.Module):\n",
    "    def __init__(self, CFG, config_path = None, pretrained = False):\n",
    "        super().__init__()\n",
    "        self.CFG = CFG\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(CFG.model, ouput_hidden_states = True)\n",
    "            self.config.save_pretrained(CFG.OUTPUT_DIR + 'config')\n",
    "            self.config.hidden_dropout = 0.\n",
    "            self.config.hidden_dropout_prob = 0.\n",
    "            self.config.attention_dropout = 0.\n",
    "            self.config.attention_probs_dropout_prob = 0.\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "            \n",
    "#         LOGGER.info(self.config)\n",
    "        \n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(CFG.model, config=self.config)\n",
    "            self.model.save_pretrained(CFG.OUTPUT_DIR + 'model')\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "            \n",
    "        if self.CFG.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "            \n",
    "        if CFG.pooling == 'mean':\n",
    "            self.pool = MeanPooling()\n",
    "        elif CFG.pooling == 'max':\n",
    "            self.pool = MaxPooling()\n",
    "        elif CFG.pooling == 'min':\n",
    "            self.pool = MinPooling()\n",
    "        elif CFG.pooling == 'attention':\n",
    "            self.pool = AttentionPooling(self.config.hidden_size)\n",
    "        elif CFG.pooling == 'weightedlayer':\n",
    "            self.pool = WeightedLayerPooling(self.config.num_hidden_layers, layer_start = CFG.layer_start, layer_weights = None)        \n",
    "        \n",
    "        self.fc = nn.Linear(self.config.hidden_size, self.CFG.n_targets)\n",
    "        self._init_weights(self.fc)\n",
    "        \n",
    "        if 'deberta-v2-xxlarge' in CFG.model:\n",
    "            self.model.embeddings.requires_grad_(False)\n",
    "            self.model.encoder.layer[:24].requires_grad_(False)\n",
    "        if 'deberta-v2-xlarge' in CFG.model:\n",
    "            self.model.embeddings.requires_grad_(False)\n",
    "            self.model.encoder.layer[:12].requires_grad_(False)\n",
    "        if 'funnel-transformer-xlarge' in CFG.model:\n",
    "            self.model.embeddings.requires_grad_(False)\n",
    "            self.model.encoder.blocks[:1].requires_grad_(False)\n",
    "        if 'funnel-transformer-large' in CFG.model:\n",
    "            self.model.embeddings.requires_grad_(False)\n",
    "            self.model.encoder.blocks[:1].requires_grad_(False)\n",
    "        if 'deberta-large' in CFG.model:\n",
    "            self.model.embeddings.requires_grad_(False)\n",
    "            self.model.encoder.layer[:16].requires_grad_(False)\n",
    "        if 'deberta-xlarge' in CFG.model:\n",
    "            self.model.embeddings.requires_grad_(False)\n",
    "            self.model.encoder.layer[:36].requires_grad_(False)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if CFG.init_weight == 'normal':\n",
    "                module.weight.data.normal_(mean = 0.0, std = self.config.initializer_range)\n",
    "            elif CFG.init_weight == 'xavier_uniform':\n",
    "                module.weight.data = nn.init.xavier_uniform_(module.weight.data)\n",
    "            elif CFG.init_weight == 'xavier_normal':\n",
    "                module.weight.data = nn.init.xavier_normal_(module.weight.data)\n",
    "            elif CFG.init_weight == 'kaiming_uniform':\n",
    "                module.weight.data = nn.init.kaiming_uniform_(module.weight.data)\n",
    "            elif CFG.init_weight == 'kaiming_normal':\n",
    "                module.weight.data = nn.init.kaiming_normal_(module.weight.data)\n",
    "            elif CFG.init_weight == 'orthogonal':\n",
    "                module.weight.data = nn.init.orthogonal_(module.weight.data)\n",
    "                \n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            if CFG.init_weight == 'normal':\n",
    "                module.weight.data.normal_(mean = 0.0, std = self.config.initializer_range)\n",
    "            elif CFG.init_weight == 'xavier_uniform':\n",
    "                module.weight.data = nn.init.xavier_uniform_(module.weight.data)\n",
    "            elif CFG.init_weight == 'xavier_normal':\n",
    "                module.weight.data = nn.init.xavier_normal_(module.weight.data)\n",
    "            elif CFG.init_weight == 'kaiming_uniform':\n",
    "                module.weight.data = nn.init.kaiming_uniform_(module.weight.data)\n",
    "            elif CFG.init_weight == 'kaiming_normal':\n",
    "                module.weight.data = nn.init.kaiming_normal_(module.weight.data)\n",
    "            elif CFG.init_weight == 'orthogonal':\n",
    "                module.weight.data = nn.init.orthogonal_(module.weight.data)\n",
    "                \n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        if CFG.pooling != 'weightedlayer':\n",
    "            last_hidden_states = outputs[0]\n",
    "            feature = self.pool(last_hidden_states, inputs['attention_mask'])\n",
    "        else:\n",
    "            all_layer_embeddings = outputs[1]\n",
    "            feature = self.pool(all_layer_embeddings)\n",
    "            \n",
    "        return feature\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        outout = self.fc(feature)\n",
    "        return outout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21375bfd",
   "metadata": {
    "id": "37df1158",
    "papermill": {
     "duration": 0.009037,
     "end_time": "2022-11-22T04:17:54.456403",
     "exception": false,
     "start_time": "2022-11-22T04:17:54.447366",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train\n",
    "* Re-initializing upper layer (normal, xavier_uniform, xavier_normal, kaiming_uniform, kaiming_normal, orthogonal) \n",
    "* Layer-Wise Learning Rate Dacay (https://www.kaggle.com/code/rhtsingh/on-stability-of-few-sample-transformer-fine-tuning?scriptVersionId=67176591&cellId=29)\n",
    "* Loss function, SmoothL1 or RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64727ffe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:17:54.476809Z",
     "iopub.status.busy": "2022-11-22T04:17:54.476079Z",
     "iopub.status.idle": "2022-11-22T04:17:54.513073Z",
     "shell.execute_reply": "2022-11-22T04:17:54.512105Z"
    },
    "id": "dd13fa6f",
    "papermill": {
     "duration": 0.049834,
     "end_time": "2022-11-22T04:17:54.515384",
     "exception": false,
     "start_time": "2022-11-22T04:17:54.465550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def re_initializing_layer(model, config, layer_num):\n",
    "    for module in model.model.encoder.layer[-layer_num:].modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if CFG.init_weight == 'normal':\n",
    "                module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
    "            elif CFG.init_weight == 'xavier_uniform':\n",
    "                module.weight.data = nn.init.xavier_uniform_(module.weight.data)\n",
    "            elif CFG.init_weight == 'xavier_normal':\n",
    "                module.weight.data = nn.init.xavier_normal_(module.weight.data)\n",
    "            elif CFG.init_weight == 'kaiming_uniform':\n",
    "                module.weight.data = nn.init.kaiming_uniform_(module.weight.data)\n",
    "            elif CFG.init_weight == 'kaiming_normal':\n",
    "                module.weight.data = nn.init.kaiming_normal_(module.weight.data)\n",
    "            elif CFG.init_weight == 'orthogonal':\n",
    "                module.weight.data = nn.init.orthogonal_(module.weight.data) \n",
    "                \n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            if CFG.init_weight == 'normal':\n",
    "                module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
    "            elif CFG.init_weight == 'xavier_uniform':\n",
    "                module.weight.data = nn.init.xavier_uniform_(module.weight.data)\n",
    "            elif CFG.init_weight == 'xavier_normal':\n",
    "                module.weight.data = nn.init.xavier_normal_(module.weight.data)\n",
    "            elif CFG.init_weight == 'kaiming_uniform':\n",
    "                module.weight.data = nn.init.kaiming_uniform_(module.weight.data)\n",
    "            elif CFG.init_weight == 'kaiming_normal':\n",
    "                module.weight.data = nn.init.kaiming_normal_(module.weight.data)\n",
    "            elif CFG.init_weight == 'orthogonal':\n",
    "                module.weight.data = nn.init.orthogonal_(module.weight.data)\n",
    "                \n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    return model   \n",
    "\n",
    "def train_loop(folds, fold):\n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "    \n",
    "    train_folds = folds[folds['fold'] != fold].reset_index(drop = True)\n",
    "    valid_folds = folds[folds['fold'] == fold].reset_index(drop = True)\n",
    "    valid_labels = valid_folds[CFG.target_cols].values\n",
    "    \n",
    "    train_dataset = FB3TrainDataset(CFG, train_folds)\n",
    "    valid_dataset = FB3TrainDataset(CFG, valid_folds)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size = CFG.batch_size,\n",
    "                              shuffle = True, \n",
    "                              num_workers = CFG.num_workers,\n",
    "                              pin_memory = True, \n",
    "                              drop_last = True\n",
    "                             )\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size = CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers,\n",
    "                              pin_memory=True, \n",
    "                              drop_last=False)\n",
    "\n",
    "    model = FB3Model(CFG, config_path = None, pretrained = True)\n",
    "    if CFG.reinit:\n",
    "        model = re_initializing_layer(model, model.config, CFG.reinit_n)\n",
    "        \n",
    "    #os.makedirs(CFG.OUTPUT_DIR + 'config/', exist_ok = True)\n",
    "    #torch.save(model.config, CFG.OUTPUT_DIR + 'config/config.pth')\n",
    "    model.to(CFG.device)\n",
    "    \n",
    "    def get_optimizer_params(model,\n",
    "                             encoder_lr,\n",
    "                             decoder_lr,\n",
    "                             weight_decay=0.0):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr,\n",
    "             'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr,\n",
    "             'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "             'lr': decoder_lr,\n",
    "             'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "    \n",
    "    #llrd\n",
    "    def get_optimizer_grouped_parameters(model, \n",
    "                                         layerwise_lr,\n",
    "                                         layerwise_weight_decay,\n",
    "                                         layerwise_lr_decay):\n",
    "        \n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        # initialize lr for task specific layer\n",
    "        optimizer_grouped_parameters = [{\"params\": [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "                                         \"weight_decay\": 0.0,\n",
    "                                         \"lr\": layerwise_lr,\n",
    "                                        },]\n",
    "        # initialize lrs for every layer\n",
    "        layers = [model.model.embeddings] + list(model.model.encoder.layer)\n",
    "        layers.reverse()\n",
    "        lr = layerwise_lr\n",
    "        for layer in layers:\n",
    "            optimizer_grouped_parameters += [{\"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                                              \"weight_decay\": layerwise_weight_decay,\n",
    "                                              \"lr\": lr,\n",
    "                                             },\n",
    "                                             {\"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                                              \"weight_decay\": 0.0,\n",
    "                                              \"lr\": lr,\n",
    "                                             },]\n",
    "            lr *= layerwise_lr_decay\n",
    "        return optimizer_grouped_parameters\n",
    "    \n",
    "    if CFG.llrd:\n",
    "        from transformers import AdamW\n",
    "        grouped_optimizer_params = get_optimizer_grouped_parameters(model, \n",
    "                                                                    CFG.layerwise_lr, \n",
    "                                                                    CFG.layerwise_weight_decay, \n",
    "                                                                    CFG.layerwise_lr_decay)\n",
    "        optimizer = AdamW(grouped_optimizer_params,\n",
    "                          lr = CFG.layerwise_lr,\n",
    "                          eps = CFG.layerwise_adam_epsilon,\n",
    "                          correct_bias = not CFG.layerwise_use_bertadam)\n",
    "    else:\n",
    "        from torch.optim import AdamW\n",
    "        optimizer_parameters = get_optimizer_params(model,\n",
    "                                                    encoder_lr=CFG.encoder_lr, \n",
    "                                                    decoder_lr=CFG.decoder_lr,\n",
    "                                                    weight_decay=CFG.weight_decay)\n",
    "        optimizer = AdamW(optimizer_parameters, \n",
    "                          lr=CFG.encoder_lr,\n",
    "                          eps=CFG.eps,\n",
    "                          betas=CFG.betas)\n",
    "    \n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        if cfg.scheduler == 'linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, \n",
    "                num_warmup_steps = cfg.num_warmup_steps, \n",
    "                num_training_steps = num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler == 'cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, \n",
    "                num_warmup_steps = cfg.num_warmup_steps, \n",
    "                num_training_steps = num_train_steps,\n",
    "                num_cycles = cfg.num_cycles\n",
    "            )\n",
    "        return scheduler\n",
    "    \n",
    "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "    \n",
    "    if CFG.loss_func == 'SmoothL1':\n",
    "        criterion = nn.SmoothL1Loss(reduction='mean')\n",
    "    elif CFG.loss_func == 'RMSE':\n",
    "        criterion = RMSELoss(reduction='mean')\n",
    "    \n",
    "    best_score = np.inf\n",
    "    best_train_loss = np.inf\n",
    "    best_val_loss = np.inf\n",
    "    \n",
    "    epoch_list = []\n",
    "    epoch_avg_loss_list = []\n",
    "    epoch_avg_val_loss_list = []\n",
    "    epoch_score_list = []\n",
    "    epoch_scores_list = []\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, CFG.device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, CFG.device)\n",
    "        \n",
    "        # scoring\n",
    "        score, scores = get_score(valid_labels, predictions)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {scores}')\n",
    "        \n",
    "        epoch_list.append(epoch+1)\n",
    "        epoch_avg_loss_list.append(avg_loss)\n",
    "        epoch_avg_val_loss_list.append(avg_val_loss)\n",
    "        epoch_score_list.append(score)\n",
    "        epoch_scores_list.append(scores)\n",
    "        \n",
    "        if best_score > score:\n",
    "            best_score = score\n",
    "            best_train_loss = avg_loss\n",
    "            best_val_loss = avg_val_loss\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                        CFG.OUTPUT_DIR + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n",
    "            \n",
    "        if CFG.save_all_models:\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                        CFG.OUTPUT_DIR + f\"{CFG.model.replace('/', '-')}_fold{fold}_epoch{epoch + 1}.pth\")\n",
    "\n",
    "    predictions = torch.load(CFG.OUTPUT_DIR + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n",
    "                             map_location = torch.device('cpu'))['predictions']\n",
    "    valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n",
    "    \n",
    "    df_epoch = pd.DataFrame({'epoch' : epoch_list,\n",
    "                             'MCRMSE' : epoch_score_list,\n",
    "                             'train_loss' : epoch_avg_loss_list, \n",
    "                             'val_loss' : epoch_avg_val_loss_list})\n",
    "    df_scores = pd.DataFrame(epoch_scores_list)\n",
    "    df_scores.columns = CFG.target_cols\n",
    "    \n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return best_train_loss, best_val_loss, valid_folds, pd.concat([df_epoch, df_scores], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8fdffe",
   "metadata": {
    "id": "1880b509",
    "papermill": {
     "duration": 0.008872,
     "end_time": "2022-11-22T04:17:54.533793",
     "exception": false,
     "start_time": "2022-11-22T04:17:54.524921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Run !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cf42f7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:17:54.553924Z",
     "iopub.status.busy": "2022-11-22T04:17:54.553569Z",
     "iopub.status.idle": "2022-11-22T04:17:54.565926Z",
     "shell.execute_reply": "2022-11-22T04:17:54.564945Z"
    },
    "id": "68c85b19",
    "outputId": "77d93ad6-3c13-4089-cdaa-4d86da94d97f",
    "papermill": {
     "duration": 0.024919,
     "end_time": "2022-11-22T04:17:54.568082",
     "exception": false,
     "start_time": "2022-11-22T04:17:54.543163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_result(oof_df, fold, best_train_loss, best_val_loss):\n",
    "    labels = oof_df[CFG.target_cols].values\n",
    "    preds = oof_df[[f\"pred_{c}\" for c in CFG.target_cols]].values\n",
    "    score, scores = get_score(labels, preds)\n",
    "    LOGGER.info(f'Score: {score:<.4f}  Scores: {scores}')\n",
    "    _output_log = pd.DataFrame([CFG.identifier, CFG.model, CFG.cv_seed, CFG.seed, fold, 'best', score, best_train_loss, best_val_loss] + scores).T\n",
    "    _output_log.columns = ['file', 'model', 'cv_seed', 'seed', 'fold', 'epoch', 'MCRMSE', 'train_loss', 'val_loss'] + CFG.target_cols\n",
    "    return _output_log\n",
    "\n",
    "if CFG.train:\n",
    "    output_log = pd.DataFrame()\n",
    "    oof_df = pd.DataFrame()\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    for fold in range(CFG.n_fold):\n",
    "        if fold in CFG.trn_fold:\n",
    "            best_train_loss, best_val_loss, _oof_df, df_epoch_scores = train_loop(CFG.df_train, fold)\n",
    "            train_loss_list.append(best_train_loss)\n",
    "            val_loss_list.append(best_val_loss)\n",
    "            oof_df = pd.concat([oof_df, _oof_df])\n",
    "            LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "\n",
    "            df_epoch_scores['file'] = CFG.identifier\n",
    "            df_epoch_scores['model'] = CFG.model\n",
    "            df_epoch_scores['cv_seed'] = CFG.cv_seed\n",
    "            df_epoch_scores['seed'] = CFG.seed\n",
    "            df_epoch_scores['fold'] = fold\n",
    "            df_epoch_scores = df_epoch_scores[['file', 'model', 'cv_seed', 'seed', 'fold', 'epoch', 'MCRMSE', 'train_loss', 'val_loss'] + CFG.target_cols]\n",
    "\n",
    "            _output_log = get_result(_oof_df, fold, best_train_loss, best_val_loss)\n",
    "            output_log = pd.concat([output_log, df_epoch_scores, _output_log])\n",
    "\n",
    "    oof_df = oof_df.reset_index(drop=True)\n",
    "    LOGGER.info(f\"========== CV ==========\")\n",
    "    _output_log = get_result(oof_df, 'OOF', np.mean(train_loss_list), np.mean(val_loss_list))\n",
    "    output_log = pd.concat([output_log, _output_log])\n",
    "    output_log.to_csv(f'{CFG.identifier}.csv', index=False)\n",
    "    oof_df.to_pickle(CFG.OUTPUT_DIR+'oof_df.pkl', protocol = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f29ff375",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:17:54.589075Z",
     "iopub.status.busy": "2022-11-22T04:17:54.587506Z",
     "iopub.status.idle": "2022-11-22T04:17:54.592560Z",
     "shell.execute_reply": "2022-11-22T04:17:54.591607Z"
    },
    "id": "p0gF5LrWA0vk",
    "papermill": {
     "duration": 0.017371,
     "end_time": "2022-11-22T04:17:54.594845",
     "exception": false,
     "start_time": "2022-11-22T04:17:54.577474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# out=pd.read_csv(f\"/{CFG.identifier}.csv\")\n",
    "# out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7931d3e",
   "metadata": {
    "papermill": {
     "duration": 0.009121,
     "end_time": "2022-11-22T04:17:54.613687",
     "exception": false,
     "start_time": "2022-11-22T04:17:54.604566",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16d7e57a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:17:54.634318Z",
     "iopub.status.busy": "2022-11-22T04:17:54.633386Z",
     "iopub.status.idle": "2022-11-22T04:17:54.654658Z",
     "shell.execute_reply": "2022-11-22T04:17:54.653661Z"
    },
    "papermill": {
     "duration": 0.033783,
     "end_time": "2022-11-22T04:17:54.656912",
     "exception": false,
     "start_time": "2022-11-22T04:17:54.623129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beea26b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:17:54.677298Z",
     "iopub.status.busy": "2022-11-22T04:17:54.676464Z",
     "iopub.status.idle": "2022-11-22T04:17:55.009650Z",
     "shell.execute_reply": "2022-11-22T04:17:55.008679Z"
    },
    "papermill": {
     "duration": 0.345791,
     "end_time": "2022-11-22T04:17:55.012075",
     "exception": false,
     "start_time": "2022-11-22T04:17:54.666284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TESTCFG:\n",
    "    num_workers=4\n",
    "    path=f\"/{CFG.identifier}\"\n",
    "    config_path=f\"{path}/config/config.json\"\n",
    "    model=\"microsoft/deberta-v3-base\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(f\"{path}/tokenizer\")\n",
    "    gradient_checkpointing=False\n",
    "    batch_size = 24#4\n",
    "    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    seed=42\n",
    "    n_fold=3\n",
    "    trn_fold=list(range(n_fold))\n",
    "    pooling = 'mean'\n",
    "    layer_start = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f498b7c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:17:55.031630Z",
     "iopub.status.busy": "2022-11-22T04:17:55.030762Z",
     "iopub.status.idle": "2022-11-22T04:17:55.202976Z",
     "shell.execute_reply": "2022-11-22T04:17:55.201720Z"
    },
    "papermill": {
     "duration": 0.184362,
     "end_time": "2022-11-22T04:17:55.205335",
     "exception": false,
     "start_time": "2022-11-22T04:17:55.020973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.4553  Scores: [0.48717903549901637, 0.4478159530447343, 0.4201908637037462, 0.4544878345655634, 0.47175101457474405, 0.4503039918436176]\n"
     ]
    }
   ],
   "source": [
    "oof_df = pd.read_pickle(TESTCFG.path + '/oof_df.pkl')\n",
    "labels = oof_df[TESTCFG.target_cols].values\n",
    "preds = oof_df[[f\"pred_{c}\" for c in TESTCFG.target_cols]].values\n",
    "score, scores = get_score(labels, preds)\n",
    "print(f'Score: {score:<.4f}  Scores: {scores}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c562e81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:17:55.224744Z",
     "iopub.status.busy": "2022-11-22T04:17:55.224067Z",
     "iopub.status.idle": "2022-11-22T04:17:55.256527Z",
     "shell.execute_reply": "2022-11-22T04:17:55.255679Z"
    },
    "papermill": {
     "duration": 0.044545,
     "end_time": "2022-11-22T04:17:55.258585",
     "exception": false,
     "start_time": "2022-11-22T04:17:55.214040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(CFG.test_file)\n",
    "submission = pd.read_csv(CFG.submission_file)\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"(\\n|\\r)+\",\" \",text.strip())\n",
    "    text = re.sub(r\"\\s+\",\" \",text)\n",
    "    return text\n",
    "test['text'] = test['full_text'].apply(clean_text)\n",
    "# sort by length to speed up inference\n",
    "test['tokenize_length'] = [len(TESTCFG.tokenizer(text)['input_ids']) for text in test['text'].values]\n",
    "test = test.sort_values('tokenize_length', ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4a368a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:17:55.277258Z",
     "iopub.status.busy": "2022-11-22T04:17:55.276357Z",
     "iopub.status.idle": "2022-11-22T04:17:55.283302Z",
     "shell.execute_reply": "2022-11-22T04:17:55.282403Z"
    },
    "papermill": {
     "duration": 0.018115,
     "end_time": "2022-11-22T04:17:55.285268",
     "exception": false,
     "start_time": "2022-11-22T04:17:55.267153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_input(cfg, text):\n",
    "    inputs = cfg.tokenizer.encode_plus(\n",
    "        text, \n",
    "        return_tensors=None, \n",
    "        add_special_tokens=True, \n",
    "        #max_length=CFG.max_len,\n",
    "        #pad_to_max_length=True,\n",
    "        #truncation=True\n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['text'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg, self.texts[item])\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b5ecc10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:17:55.303392Z",
     "iopub.status.busy": "2022-11-22T04:17:55.302624Z",
     "iopub.status.idle": "2022-11-22T04:17:55.308603Z",
     "shell.execute_reply": "2022-11-22T04:17:55.307738Z"
    },
    "papermill": {
     "duration": 0.017088,
     "end_time": "2022-11-22T04:17:55.310595",
     "exception": false,
     "start_time": "2022-11-22T04:17:55.293507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min = 1e-9)\n",
    "        mean_embeddings = sum_embeddings/sum_mask\n",
    "        return mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e302c652",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:17:55.328519Z",
     "iopub.status.busy": "2022-11-22T04:17:55.328262Z",
     "iopub.status.idle": "2022-11-22T04:17:55.342187Z",
     "shell.execute_reply": "2022-11-22T04:17:55.341347Z"
    },
    "papermill": {
     "duration": 0.02493,
     "end_time": "2022-11-22T04:17:55.344102",
     "exception": false,
     "start_time": "2022-11-22T04:17:55.319172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n",
    "            self.config.hidden_dropout = 0.\n",
    "            self.config.hidden_dropout_prob = 0.\n",
    "            self.config.attention_dropout = 0.\n",
    "            self.config.attention_probs_dropout_prob = 0.\n",
    "\n",
    "        else:\n",
    "            #self.config = torch.load(config_path)\n",
    "            self.config = AutoConfig.from_pretrained(config_path, output_hidden_states=True)\n",
    "\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel.from_config(self.config)\n",
    "        if self.cfg.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        if CFG.pooling == 'mean':\n",
    "            self.pool = MeanPooling()\n",
    "        elif CFG.pooling == 'max':\n",
    "            self.pool = MaxPooling()\n",
    "        elif CFG.pooling == 'min':\n",
    "            self.pool = MinPooling()\n",
    "        elif CFG.pooling == 'attention':\n",
    "            self.pool = AttentionPooling(self.config.hidden_size)\n",
    "        elif CFG.pooling == 'weightedlayer':\n",
    "            self.pool = WeightedLayerPooling(self.config.num_hidden_layers, layer_start = CFG.layer_start, layer_weights = None)        \n",
    "\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 6)\n",
    "        self._init_weights(self.fc)\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        feature = self.pool(last_hidden_states, inputs['attention_mask'])\n",
    "        return feature\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(feature)\n",
    "        return output                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fee0576e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:17:55.362494Z",
     "iopub.status.busy": "2022-11-22T04:17:55.362196Z",
     "iopub.status.idle": "2022-11-22T04:17:55.368199Z",
     "shell.execute_reply": "2022-11-22T04:17:55.367236Z"
    },
    "papermill": {
     "duration": 0.01746,
     "end_time": "2022-11-22T04:17:55.370017",
     "exception": false,
     "start_time": "2022-11-22T04:17:55.352557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# inference\n",
    "# ====================================================\n",
    "def inference_fn(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02fc8063",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:17:55.387998Z",
     "iopub.status.busy": "2022-11-22T04:17:55.387367Z",
     "iopub.status.idle": "2022-11-22T04:18:47.567334Z",
     "shell.execute_reply": "2022-11-22T04:18:47.566030Z"
    },
    "papermill": {
     "duration": 52.192615,
     "end_time": "2022-11-22T04:18:47.571016",
     "exception": false,
     "start_time": "2022-11-22T04:17:55.378401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OUTPUT_DIR: ./20221121-172724-deberta-v3-base/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c589b14a2b74620b5716e302ee03e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8df26ce7934974ab41eeaf4643c0f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d247cfd3c05a43f0a84772502b5c6a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if CFG.test:\n",
    "    LOGGER = get_logger()\n",
    "    LOGGER.info(f'OUTPUT_DIR: {CFG.OUTPUT_DIR}')\n",
    "    test_dataset = TestDataset(TESTCFG, test)\n",
    "    test_loader = DataLoader(test_dataset,\n",
    "                             batch_size=TESTCFG.batch_size,\n",
    "                             shuffle=False,\n",
    "                             collate_fn=DataCollatorWithPadding(tokenizer=TESTCFG.tokenizer, padding='longest'),\n",
    "                             num_workers=TESTCFG.num_workers, pin_memory=True, drop_last=False)\n",
    "    predictions = []\n",
    "    for fold in TESTCFG.trn_fold:\n",
    "        model = CustomModel(TESTCFG, config_path=TESTCFG.config_path, pretrained=False)\n",
    "        state = torch.load(TESTCFG.path + \"/\"+f\"{TESTCFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n",
    "                           map_location=torch.device('cpu'))\n",
    "                           #map_location='cuda:0')\n",
    "        model.load_state_dict(state['model'])\n",
    "        prediction = inference_fn(test_loader, model, CFG.device)\n",
    "        predictions.append(prediction)\n",
    "        del model, state, prediction; gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    predictions = np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7f9817",
   "metadata": {
    "papermill": {
     "duration": 0.021393,
     "end_time": "2022-11-22T04:18:47.616192",
     "exception": false,
     "start_time": "2022-11-22T04:18:47.594799",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4fea81d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:18:47.652053Z",
     "iopub.status.busy": "2022-11-22T04:18:47.651660Z",
     "iopub.status.idle": "2022-11-22T04:18:47.685458Z",
     "shell.execute_reply": "2022-11-22T04:18:47.684285Z"
    },
    "papermill": {
     "duration": 0.051035,
     "end_time": "2022-11-22T04:18:47.688613",
     "exception": false,
     "start_time": "2022-11-22T04:18:47.637578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000C359D63E</td>\n",
       "      <td>2.890374</td>\n",
       "      <td>2.738518</td>\n",
       "      <td>3.095811</td>\n",
       "      <td>2.964157</td>\n",
       "      <td>2.709416</td>\n",
       "      <td>2.686949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000BAD50D026</td>\n",
       "      <td>2.588973</td>\n",
       "      <td>2.373482</td>\n",
       "      <td>2.689124</td>\n",
       "      <td>2.336511</td>\n",
       "      <td>2.106308</td>\n",
       "      <td>2.522703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00367BB2546B</td>\n",
       "      <td>3.450903</td>\n",
       "      <td>3.279081</td>\n",
       "      <td>3.457369</td>\n",
       "      <td>3.465577</td>\n",
       "      <td>3.354650</td>\n",
       "      <td>3.286624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n",
       "0  0000C359D63E  2.890374  2.738518    3.095811     2.964157  2.709416   \n",
       "1  000BAD50D026  2.588973  2.373482    2.689124     2.336511  2.106308   \n",
       "2  00367BB2546B  3.450903  3.279081    3.457369     3.465577  3.354650   \n",
       "\n",
       "   conventions  \n",
       "0     2.686949  \n",
       "1     2.522703  \n",
       "2     3.286624  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n"
     ]
    }
   ],
   "source": [
    "test[CFG.target_cols] = predictions.clip(1, 5)\n",
    "submission = submission.drop(columns=CFG.target_cols).merge(test[['text_id'] + CFG.target_cols], on='text_id', how='left')\n",
    "display(submission.head())\n",
    "submission[['text_id'] + CFG.target_cols].to_csv('submission.csv', index=False)\n",
    "print(\"completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a192afc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-22T04:18:47.709190Z",
     "iopub.status.busy": "2022-11-22T04:18:47.708854Z",
     "iopub.status.idle": "2022-11-22T04:18:47.713627Z",
     "shell.execute_reply": "2022-11-22T04:18:47.712398Z"
    },
    "papermill": {
     "duration": 0.018021,
     "end_time": "2022-11-22T04:18:47.716566",
     "exception": false,
     "start_time": "2022-11-22T04:18:47.698545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DONE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 71.475837,
   "end_time": "2022-11-22T04:18:50.447152",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-22T04:17:38.971315",
   "version": "2.3.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "061906d520f444328b464fb7670c33bb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "09bfe265e4e443b3a0e76414bc5fcbf5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "145db698515d49eab1c979c81eca0186": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "164ce1f9005348f88787df8c537c3c82": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1ce77f2005da408993925f19c504a08a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "20b65a0a50f9475b861afc79e0ce58c5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "291e77d070e8449d91c9c47a6b24252c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4c589b14a2b74620b5716e302ee03e99": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a9d3e218083247c7a7098f2d411f1d8f",
        "IPY_MODEL_9773ab2caa024046adaab42541f22629",
        "IPY_MODEL_cee082db49b44b089bcdcdfdb2c16ba9"
       ],
       "layout": "IPY_MODEL_8572563f861b43d09b8776929c27a28c"
      }
     },
     "5cb7d20fe8a94e10ad30b010f3975ece": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "66b4942f586947cc85923744a596e865": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c33382c0abd641f28486cd3c90d837d2",
       "placeholder": "​",
       "style": "IPY_MODEL_818263f5d5094783886d2de9074264ee",
       "value": "100%"
      }
     },
     "818263f5d5094783886d2de9074264ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "84ceac25eb7a4a9d99254578cb76d387": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "856449cf4af24c2e993d5e816a28e3a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "8572563f861b43d09b8776929c27a28c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8a10b57fabd64524a52a97e67441a6af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8be4eb5ab05d4a0195c5a47642ade81d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_164ce1f9005348f88787df8c537c3c82",
       "placeholder": "​",
       "style": "IPY_MODEL_ba812488fdae439f9dcedf0d8c2c4518",
       "value": " 1/1 [00:00&lt;00:00,  1.61it/s]"
      }
     },
     "8e6823f602d143c492cb1ee5a752452f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9773ab2caa024046adaab42541f22629": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_84ceac25eb7a4a9d99254578cb76d387",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_856449cf4af24c2e993d5e816a28e3a7",
       "value": 1.0
      }
     },
     "993e601bb7e141118181ee9e0f1900fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_09bfe265e4e443b3a0e76414bc5fcbf5",
       "placeholder": "​",
       "style": "IPY_MODEL_b75562c5d15f4b73bcfb6eabd30fce66",
       "value": "100%"
      }
     },
     "a01e706bd742441d8732d08134c90dff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a9d3e218083247c7a7098f2d411f1d8f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8e6823f602d143c492cb1ee5a752452f",
       "placeholder": "​",
       "style": "IPY_MODEL_145db698515d49eab1c979c81eca0186",
       "value": "100%"
      }
     },
     "ac8ab0d2788d4de683e5371b6ede36d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b61841357f574958b6929c35f423ee7c",
       "placeholder": "​",
       "style": "IPY_MODEL_8a10b57fabd64524a52a97e67441a6af",
       "value": " 1/1 [00:00&lt;00:00,  1.59it/s]"
      }
     },
     "b61841357f574958b6929c35f423ee7c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b75562c5d15f4b73bcfb6eabd30fce66": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ba812488fdae439f9dcedf0d8c2c4518": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c33382c0abd641f28486cd3c90d837d2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cc08ee47cd5c44859f0796355d6751ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_20b65a0a50f9475b861afc79e0ce58c5",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a01e706bd742441d8732d08134c90dff",
       "value": 1.0
      }
     },
     "cee082db49b44b089bcdcdfdb2c16ba9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d4de89cdb3774c9cb3b3cb0d10595522",
       "placeholder": "​",
       "style": "IPY_MODEL_1ce77f2005da408993925f19c504a08a",
       "value": " 1/1 [00:01&lt;00:00,  1.58s/it]"
      }
     },
     "cf8df26ce7934974ab41eeaf4643c0f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_66b4942f586947cc85923744a596e865",
        "IPY_MODEL_cc08ee47cd5c44859f0796355d6751ed",
        "IPY_MODEL_ac8ab0d2788d4de683e5371b6ede36d6"
       ],
       "layout": "IPY_MODEL_f44bc030174b45e1b9acf6cd035aedb8"
      }
     },
     "d247cfd3c05a43f0a84772502b5c6a01": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_993e601bb7e141118181ee9e0f1900fe",
        "IPY_MODEL_eb55326471e4439ca678bb668618aed7",
        "IPY_MODEL_8be4eb5ab05d4a0195c5a47642ade81d"
       ],
       "layout": "IPY_MODEL_291e77d070e8449d91c9c47a6b24252c"
      }
     },
     "d4de89cdb3774c9cb3b3cb0d10595522": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eb55326471e4439ca678bb668618aed7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_061906d520f444328b464fb7670c33bb",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5cb7d20fe8a94e10ad30b010f3975ece",
       "value": 1.0
      }
     },
     "f44bc030174b45e1b9acf6cd035aedb8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

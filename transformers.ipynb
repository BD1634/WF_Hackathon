{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3aeffc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T17:44:32.732883Z",
     "iopub.status.busy": "2022-11-29T17:44:32.732051Z",
     "iopub.status.idle": "2022-11-29T17:44:40.114606Z",
     "shell.execute_reply": "2022-11-29T17:44:40.113629Z"
    },
    "id": "85ae051a",
    "outputId": "7417e36b-88ee-4670-f3e1-b8c07171ca96",
    "papermill": {
     "duration": 7.396807,
     "end_time": "2022-11-29T17:44:40.117387",
     "exception": false,
     "start_time": "2022-11-29T17:44:32.720580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers.__version__: 4.20.1\n",
      "tokenizers.__version__: 0.12.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "from distutils.util import strtobool\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import datetime\n",
    "import transformers\n",
    "import tokenizers\n",
    "print(f'transformers.__version__: {transformers.__version__}')\n",
    "print(f'tokenizers.__version__: {tokenizers.__version__}')\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "os.environ['TOKENIZERS_PARALLELISM']='true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c413d3b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T17:44:40.135927Z",
     "iopub.status.busy": "2022-11-29T17:44:40.135458Z",
     "iopub.status.idle": "2022-11-29T17:44:40.223738Z",
     "shell.execute_reply": "2022-11-29T17:44:40.222591Z"
    },
    "papermill": {
     "duration": 0.099853,
     "end_time": "2022-11-29T17:44:40.226039",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.126186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/pretran-model/model/runs/Nov28_19-07-29_881c264b9fa9/events.out.tfevents.1669662562.881c264b9fa9.23.0\n",
      "/kaggle/input/pretran-model/model/runs/Nov28_19-07-29_881c264b9fa9/events.out.tfevents.1669666880.881c264b9fa9.23.2\n",
      "/kaggle/input/pretran-model/model/runs/Nov28_19-07-29_881c264b9fa9/1669662562.9408693/events.out.tfevents.1669662562.881c264b9fa9.23.1\n",
      "/kaggle/input/pretran-model/model/checkpoint-1131/spm.model\n",
      "/kaggle/input/pretran-model/model/checkpoint-1131/config.json\n",
      "/kaggle/input/pretran-model/model/checkpoint-1131/trainer_state.json\n",
      "/kaggle/input/pretran-model/model/checkpoint-1131/training_args.bin\n",
      "/kaggle/input/pretran-model/model/checkpoint-1131/tokenizer_config.json\n",
      "/kaggle/input/pretran-model/model/checkpoint-1131/pytorch_model.bin\n",
      "/kaggle/input/pretran-model/model/checkpoint-1131/scaler.pt\n",
      "/kaggle/input/pretran-model/model/checkpoint-1131/scheduler.pt\n",
      "/kaggle/input/pretran-model/model/checkpoint-1131/special_tokens_map.json\n",
      "/kaggle/input/pretran-model/model/checkpoint-1131/optimizer.pt\n",
      "/kaggle/input/pretran-model/model/checkpoint-1131/rng_state.pth\n",
      "/kaggle/input/pretran-model/model/checkpoint-1131/added_tokens.json\n",
      "/kaggle/input/pretran-model/pretrained_models/microsoft/deberta-v3-base1/spm.model\n",
      "/kaggle/input/pretran-model/pretrained_models/microsoft/deberta-v3-base1/config.json\n",
      "/kaggle/input/pretran-model/pretrained_models/microsoft/deberta-v3-base1/training_args.bin\n",
      "/kaggle/input/pretran-model/pretrained_models/microsoft/deberta-v3-base1/tokenizer_config.json\n",
      "/kaggle/input/pretran-model/pretrained_models/microsoft/deberta-v3-base1/pytorch_model.bin\n",
      "/kaggle/input/pretran-model/pretrained_models/microsoft/deberta-v3-base1/special_tokens_map.json\n",
      "/kaggle/input/pretran-model/pretrained_models/microsoft/deberta-v3-base1/added_tokens.json\n",
      "/kaggle/input/pretrain/sample_submission (1).csv\n",
      "/kaggle/input/pretrain/test.csv\n",
      "/kaggle/input/pretrain/children_books.csv/children_books.csv\n",
      "/kaggle/input/feedback-prize-english-language-learning/sample_submission.csv\n",
      "/kaggle/input/feedback-prize-english-language-learning/train.csv\n",
      "/kaggle/input/feedback-prize-english-language-learning/test.csv\n",
      "/kaggle/input/fnetune/20221129-081736-/kaggle/input/pretran-model/pretrained_models/microsoft/deberta-v3-base1.csv\n",
      "/kaggle/input/fnetune/20221129-081736-/kaggle/input/pretran-model/pretrained_models/microsoft/deberta-v3-base1/-kaggle-input-pretran-model-pretrained_models-microsoft-deberta-v3-base1_fold2_best.pth\n",
      "/kaggle/input/fnetune/20221129-081736-/kaggle/input/pretran-model/pretrained_models/microsoft/deberta-v3-base1/train.log\n",
      "/kaggle/input/fnetune/20221129-081736-/kaggle/input/pretran-model/pretrained_models/microsoft/deberta-v3-base1/-kaggle-input-pretran-model-pretrained_models-microsoft-deberta-v3-base1_fold4_best.pth\n",
      "/kaggle/input/fnetune/20221129-081736-/kaggle/input/pretran-model/pretrained_models/microsoft/deberta-v3-base1/-kaggle-input-pretran-model-pretrained_models-microsoft-deberta-v3-base1_fold3_best.pth\n",
      "/kaggle/input/fnetune/20221129-081736-/kaggle/input/pretran-model/pretrained_models/microsoft/deberta-v3-base1/-kaggle-input-pretran-model-pretrained_models-microsoft-deberta-v3-base1_fold0_best.pth\n",
      "/kaggle/input/fnetune/20221129-081736-/kaggle/input/pretran-model/pretrained_models/microsoft/deberta-v3-base1/oof_df.pkl\n",
      "/kaggle/input/fnetune/20221129-081736-/kaggle/input/pretran-model/pretrained_models/microsoft/deberta-v3-base1/-kaggle-input-pretran-model-pretrained_models-microsoft-deberta-v3-base1_fold1_best.pth\n",
      "/kaggle/input/fnetune/20221129-081736-/kaggle/input/pretran-model/pretrained_models/microsoft/deberta-v3-base1/model/config.json\n",
      "/kaggle/input/fnetune/20221129-081736-/kaggle/input/pretran-model/pretrained_models/microsoft/deberta-v3-base1/model/pytorch_model.bin\n",
      "/kaggle/input/fnetune/20221129-081736-/kaggle/input/pretran-model/pretrained_models/microsoft/deberta-v3-base1/tokenizer/spm.model\n",
      "/kaggle/input/fnetune/20221129-081736-/kaggle/input/pretran-model/pretrained_models/microsoft/deberta-v3-base1/tokenizer/tokenizer.json\n",
      "/kaggle/input/fnetune/20221129-081736-/kaggle/input/pretran-model/pretrained_models/microsoft/deberta-v3-base1/tokenizer/tokenizer_config.json\n",
      "/kaggle/input/fnetune/20221129-081736-/kaggle/input/pretran-model/pretrained_models/microsoft/deberta-v3-base1/tokenizer/special_tokens_map.json\n",
      "/kaggle/input/fnetune/20221129-081736-/kaggle/input/pretran-model/pretrained_models/microsoft/deberta-v3-base1/tokenizer/added_tokens.json\n",
      "/kaggle/input/fnetune/20221129-081736-/kaggle/input/pretran-model/pretrained_models/microsoft/deberta-v3-base1/config/config.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae9a1b4",
   "metadata": {
    "id": "bd8d61dc",
    "papermill": {
     "duration": 0.009642,
     "end_time": "2022-11-29T17:44:40.244091",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.234449",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeff39cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T17:44:40.262880Z",
     "iopub.status.busy": "2022-11-29T17:44:40.262135Z",
     "iopub.status.idle": "2022-11-29T17:44:40.272379Z",
     "shell.execute_reply": "2022-11-29T17:44:40.271500Z"
    },
    "id": "a5bdb746",
    "papermill": {
     "duration": 0.021809,
     "end_time": "2022-11-29T17:44:40.274364",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.252555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    str_now = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "    train = False\n",
    "    test = True\n",
    "    debug = False\n",
    "    offline = False\n",
    "    models_path = 'feedback-models'\n",
    "    epochs = 5\n",
    "    save_all_models = False\n",
    "    competition = 'Feedback'\n",
    "    apex = True\n",
    "    print_freq = 30\n",
    "    num_workers = 4\n",
    "    model = '/kaggle/input/pretran-model/pretrained_models/microsoft/deberta-v3-base1' #If you want to train on the kaggle platform, v3-base is realistic. v3-large will time out.\n",
    "    loss_func = 'RMSE' # 'SmoothL1', 'RMSE'\n",
    "    gradient_checkpointing = True\n",
    "    scheduler = 'cosine'\n",
    "    batch_scheduler = True\n",
    "    num_cycles = 0.5\n",
    "    num_warmup_steps = 0\n",
    "    encoder_lr = 2e-5\n",
    "    decoder_lr = 2e-5\n",
    "    min_lr = 1e-6\n",
    "    #Layer-Wise Learning Rate Decay\n",
    "    llrd = True\n",
    "    layerwise_lr = 5e-5\n",
    "    layerwise_lr_decay = 0.5\n",
    "    layerwise_weight_decay = 0.01\n",
    "    layerwise_adam_epsilon = 1e-6\n",
    "    layerwise_use_bertadam = False\n",
    "    #pooling\n",
    "    pooling = 'attention' # mean, max, min, attention, weightedlayer\n",
    "    layer_start = 4\n",
    "    #init_weight\n",
    "    init_weight = 'normal' # normal, xavier_uniform, xavier_normal, kaiming_uniform, kaiming_normal, orthogonal\n",
    "    #re-init\n",
    "    reinit = True\n",
    "    reinit_n = 1\n",
    "    #adversarial\n",
    "    fgm = False\n",
    "    awp = False\n",
    "    adv_lr = 1\n",
    "    adv_eps = 0.2\n",
    "    unscale = False\n",
    "    eps = 1e-6\n",
    "    betas = (0.9, 0.999)\n",
    "    max_len = 800\n",
    "    weight_decay = 0.01\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm = 1000\n",
    "    target_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    seed = 2022\n",
    "    cv_seed = 2022\n",
    "    n_fold = 5\n",
    "    trn_fold = list(range(n_fold))\n",
    "    batch_size = 8\n",
    "    n_targets = 6\n",
    "    gpu_id = 0\n",
    "    device = f'cuda:{gpu_id}'\n",
    "    train_file = '/kaggle/input/feedback-prize-english-language-learning/train.csv'\n",
    "    test_file = '/kaggle/input/feedback-prize-english-language-learning/test.csv'\n",
    "    submission_file = '/kaggle/input/feedback-prize-english-language-learning/sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b011c687",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T17:44:40.293937Z",
     "iopub.status.busy": "2022-11-29T17:44:40.293135Z",
     "iopub.status.idle": "2022-11-29T17:44:40.300583Z",
     "shell.execute_reply": "2022-11-29T17:44:40.299205Z"
    },
    "id": "35118197",
    "outputId": "d6badf71-a8a3-4fd8-dc16-959657bfbfb1",
    "papermill": {
     "duration": 0.020264,
     "end_time": "2022-11-29T17:44:40.303234",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.282970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle/input/fnetune/20221129-081736-/kaggle/input/pretran-model/pretrained_models/microsoft/deberta-v3-base1\n"
     ]
    }
   ],
   "source": [
    "#Unique model name\n",
    "if CFG.train:\n",
    "    if len(CFG.model.split(\"/\")) == 2:\n",
    "        CFG.identifier = f'{CFG.str_now}-{CFG.model.split(\"/\")[1]}'\n",
    "    else:\n",
    "        CFG.identifier = f'{CFG.str_now}-{CFG.model}'\n",
    "else:\n",
    "    CFG.identifier = \"kaggle/input/fnetune/20221129-081736-/kaggle/input/pretran-model/pretrained_models/microsoft/deberta-v3-base1\"\n",
    "    CFG.OUTPUT_DIR = f'./20221122-054549-deberta-v3-base/'\n",
    "    CFG.log_filename = CFG.OUTPUT_DIR + 'test'\n",
    "    os.makedirs(CFG.OUTPUT_DIR, exist_ok=True)\n",
    "print(CFG.identifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c8d1b3",
   "metadata": {
    "id": "e5e7ca6a",
    "papermill": {
     "duration": 0.009006,
     "end_time": "2022-11-29T17:44:40.321633",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.312627",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Read train and split with MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11b5951f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T17:44:40.340505Z",
     "iopub.status.busy": "2022-11-29T17:44:40.340196Z",
     "iopub.status.idle": "2022-11-29T17:44:40.350380Z",
     "shell.execute_reply": "2022-11-29T17:44:40.349418Z"
    },
    "id": "b10683c9",
    "outputId": "81a12101-8e5b-42e9-e06f-9512f41d5a96",
    "papermill": {
     "duration": 0.022009,
     "end_time": "2022-11-29T17:44:40.352402",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.330393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.train:\n",
    "    CFG.df_train = pd.read_csv(CFG.train_file)\n",
    "    CFG.OUTPUT_DIR = f'./{CFG.identifier}/'\n",
    "    CFG.log_filename = CFG.OUTPUT_DIR + 'train'\n",
    "    if CFG.offline:\n",
    "        #TO DO\n",
    "        pass\n",
    "    else:\n",
    "        os.system('pip install iterative-stratification==0.1.7')\n",
    "    #CV\n",
    "#     x_train , x_val = train_test_split(CFG.df_train,random_state=42,test_size=0.3)\n",
    "    \n",
    "\n",
    "    from iterstrat.ml_stratifiers import MultilabelStratifiedKFold    \n",
    "    Fold = MultilabelStratifiedKFold(n_splits = CFG.n_fold, shuffle = True, random_state = CFG.cv_seed)\n",
    "    y = pd.get_dummies(data=CFG.df_train[CFG.target_cols], columns=CFG.target_cols)\n",
    "    for n, (train_index, val_index) in enumerate(Fold.split(X=CFG.df_train, y=y)):\n",
    "        CFG.df_train.loc[val_index, 'fold'] = int(n)\n",
    "    \n",
    "    import re\n",
    "    def clean_text(text):\n",
    "        text = re.sub(r\"(\\n|\\r)+\",\" \",text.strip())\n",
    "        text = re.sub(r\"\\s+\",\" \",text)\n",
    "        return text\n",
    "    CFG.df_train['text'] = CFG.df_train['full_text'].apply(clean_text)\n",
    "    CFG.df_train['fold'] = CFG.df_train['fold'].astype(int)\n",
    "    os.makedirs(CFG.OUTPUT_DIR, exist_ok=True)    \n",
    "    print(CFG.OUTPUT_DIR)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.trn_fold = [0]\n",
    "    if CFG.train:\n",
    "        CFG.df_train = CFG.df_train.sample(n = 100, random_state = CFG.seed).reset_index(drop=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64915af9",
   "metadata": {
    "id": "364e9c27",
    "papermill": {
     "duration": 0.00843,
     "end_time": "2022-11-29T17:44:40.369627",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.361197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c61bfe36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T17:44:40.388028Z",
     "iopub.status.busy": "2022-11-29T17:44:40.387772Z",
     "iopub.status.idle": "2022-11-29T17:44:40.407619Z",
     "shell.execute_reply": "2022-11-29T17:44:40.406772Z"
    },
    "id": "9d6dcaba",
    "papermill": {
     "duration": 0.03153,
     "end_time": "2022-11-29T17:44:40.409619",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.378089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def MCRMSE(y_trues, y_preds):\n",
    "    scores = []\n",
    "    idxes = y_trues.shape[1]\n",
    "    for i in range(idxes):\n",
    "        y_true = y_trues[:, i]\n",
    "        y_pred = y_preds[:, i]\n",
    "        score = mean_squared_error(y_true, y_pred, squared = False)\n",
    "        scores.append(score)\n",
    "    mcrmse_score = np.mean(scores)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "def get_score(y_trues, y_preds):\n",
    "    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "def get_logger(filename = CFG.log_filename):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter('%(message)s'))\n",
    "    handler2 = FileHandler(filename = f'{filename}.log')\n",
    "    handler2.setFormatter(Formatter('%(message)s'))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "def prepare_input(cfg, text):\n",
    "    inputs = cfg.tokenizer.encode_plus(\n",
    "        text,\n",
    "        return_tensors = None,\n",
    "        add_special_tokens = True,\n",
    "        max_length = cfg.max_len,\n",
    "        pad_to_max_length = True,\n",
    "        truncation = True\n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype = torch.long)\n",
    "    return inputs    \n",
    "\n",
    "def collate(inputs):\n",
    "    mask_len = int(inputs['attention_mask'].sum(axis = 1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:, :mask_len]\n",
    "    return inputs\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def update(self, val, n = 1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return f'{int(m)}m {int(s)}s'\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return f'{str(asMinutes(s))} (remain {str(asMinutes(rs))})'\n",
    "\n",
    "def seed_everything(seed = 42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, reduction = 'mean', eps = 1e-9):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss(reduction = 'none')\n",
    "        self.reduction = reduction\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n",
    "        if self.reduction == 'none':\n",
    "            loss = loss\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c1ac20",
   "metadata": {
    "id": "55633022",
    "papermill": {
     "duration": 0.00832,
     "end_time": "2022-11-29T17:44:40.426593",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.418273",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pooling\n",
    "\n",
    "* Attention pooling (https://www.kaggle.com/competitions/feedback-prize-english-language-learning/discussion/361678)\n",
    "* WeightedLayerPooling (https://www.kaggle.com/code/rhtsingh/on-stability-of-few-sample-transformer-fine-tuning?scriptVersionId=67176591&cellId=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "097308ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T17:44:40.444921Z",
     "iopub.status.busy": "2022-11-29T17:44:40.444670Z",
     "iopub.status.idle": "2022-11-29T17:44:40.460959Z",
     "shell.execute_reply": "2022-11-29T17:44:40.460119Z"
    },
    "id": "ed191bd8",
    "papermill": {
     "duration": 0.027781,
     "end_time": "2022-11-29T17:44:40.462882",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.435101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min = 1e-9)\n",
    "        mean_embeddings = sum_embeddings/sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "class MaxPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaxPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        embeddings = last_hidden_state.clone()\n",
    "        embeddings[input_mask_expanded == 0] = -1e4\n",
    "        max_embeddings, _ = torch.max(embeddings, dim = 1)\n",
    "        return max_embeddings\n",
    "    \n",
    "class MinPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MinPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        embeddings = last_hidden_state.clone()\n",
    "        embeddings[input_mask_expanded == 0] = 1e-4\n",
    "        min_embeddings, _ = torch.min(embeddings, dim = 1)\n",
    "        return min_embeddings\n",
    "\n",
    "#Attention pooling\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "        nn.Linear(in_dim, in_dim),\n",
    "        nn.LayerNorm(in_dim),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(in_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        w = self.attention(last_hidden_state).float()\n",
    "        w[attention_mask==0]=float('-inf')\n",
    "        w = torch.softmax(w,1)\n",
    "        attention_embeddings = torch.sum(w * last_hidden_state, dim=1)\n",
    "        return attention_embeddings\n",
    "\n",
    "#There may be a bug in my implementation because it does not work well.\n",
    "class WeightedLayerPooling(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n",
    "        super(WeightedLayerPooling, self).__init__()\n",
    "        self.layer_start = layer_start\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layer_weights = layer_weights if layer_weights is not None \\\n",
    "            else nn.Parameter(\n",
    "                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n",
    "            )\n",
    "\n",
    "    def forward(self, ft_all_layers):\n",
    "        all_layer_embedding = torch.stack(ft_all_layers)\n",
    "        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n",
    "\n",
    "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
    "        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
    "\n",
    "        return weighted_average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d334ef39",
   "metadata": {
    "id": "92707e50",
    "papermill": {
     "duration": 0.008292,
     "end_time": "2022-11-29T17:44:40.479887",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.471595",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fast Gradient Method (FGM)\n",
    "Reference :\n",
    "\n",
    "https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/143764"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e321f582",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T17:44:40.498333Z",
     "iopub.status.busy": "2022-11-29T17:44:40.497587Z",
     "iopub.status.idle": "2022-11-29T17:44:40.505129Z",
     "shell.execute_reply": "2022-11-29T17:44:40.504336Z"
    },
    "id": "857cb5c4",
    "papermill": {
     "duration": 0.018711,
     "end_time": "2022-11-29T17:44:40.507064",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.488353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FGM():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self, epsilon = 1., emb_name = 'word_embeddings'):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0:\n",
    "                    r_at = epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "\n",
    "    def restore(self, emb_name = 'word_embeddings'):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "            self.backup = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb67973a",
   "metadata": {
    "id": "466b0768",
    "papermill": {
     "duration": 0.008296,
     "end_time": "2022-11-29T17:44:40.523874",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.515578",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Adversarial Weight Perturbation (AWP)\n",
    "There may be a bug in my implementation because it does not work well.\n",
    "\n",
    "Reference : \n",
    "\n",
    "https://www.kaggle.com/code/wht1996/feedback-nn-train/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eabea26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T17:44:40.542448Z",
     "iopub.status.busy": "2022-11-29T17:44:40.542183Z",
     "iopub.status.idle": "2022-11-29T17:44:40.556369Z",
     "shell.execute_reply": "2022-11-29T17:44:40.555447Z"
    },
    "id": "21375aec",
    "papermill": {
     "duration": 0.025685,
     "end_time": "2022-11-29T17:44:40.558187",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.532502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AWP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        optimizer,\n",
    "        adv_param=\"weight\",\n",
    "        adv_lr=1,\n",
    "        adv_eps=0.2,\n",
    "        start_epoch=0,\n",
    "        adv_step=1,\n",
    "        scaler=None\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.adv_param = adv_param\n",
    "        self.adv_lr = adv_lr\n",
    "        self.adv_eps = adv_eps\n",
    "        self.start_epoch = start_epoch\n",
    "        self.adv_step = adv_step\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}\n",
    "        self.scaler = scaler\n",
    "        \n",
    "    def attack_backward(self, x, y, attention_mask,epoch):\n",
    "        if (self.adv_lr == 0) or (epoch < self.start_epoch):\n",
    "            return None\n",
    "\n",
    "        self._save() \n",
    "        for i in range(self.adv_step):\n",
    "            self._attack_step() \n",
    "            with torch.cuda.amp.autocast():\n",
    "                adv_loss, tr_logits = self.model(input_ids=x, attention_mask=attention_mask, labels=y)\n",
    "                adv_loss = adv_loss.mean()\n",
    "            self.optimizer.zero_grad()\n",
    "            self.scaler.scale(adv_loss).backward()\n",
    "            \n",
    "        self._restore()\n",
    "        \n",
    "    def _attack_step(self):\n",
    "        e = 1e-6\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                norm1 = torch.norm(param.grad)\n",
    "                norm2 = torch.norm(param.data.detach())\n",
    "                if norm1 != 0 and not torch.isnan(norm1):\n",
    "                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n",
    "                    param.data.add_(r_at)\n",
    "                    param.data = torch.min(\n",
    "                        torch.max(param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n",
    "                    )\n",
    "                # param.data.clamp_(*self.backup_eps[name])\n",
    "\n",
    "    def _save(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                if name not in self.backup:\n",
    "                    self.backup[name] = param.data.clone()\n",
    "                    grad_eps = self.adv_eps * param.abs().detach()\n",
    "                    self.backup_eps[name] = (\n",
    "                        self.backup[name] - grad_eps,\n",
    "                        self.backup[name] + grad_eps,\n",
    "                    )\n",
    "\n",
    "    def _restore(self,):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in self.backup:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5a455c",
   "metadata": {
    "id": "b6bdf2ac",
    "papermill": {
     "duration": 0.008672,
     "end_time": "2022-11-29T17:44:40.575361",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.566689",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train function\n",
    "* FGM\n",
    "* AWP (There may be a bug.)\n",
    "* Unscale optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f96bf29d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T17:44:40.594002Z",
     "iopub.status.busy": "2022-11-29T17:44:40.593473Z",
     "iopub.status.idle": "2022-11-29T17:44:40.607468Z",
     "shell.execute_reply": "2022-11-29T17:44:40.606652Z"
    },
    "id": "7a53dbdb",
    "papermill": {
     "duration": 0.025548,
     "end_time": "2022-11-29T17:44:40.609409",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.583861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    losses = AverageMeter()\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled = CFG.apex)\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    if CFG.fgm:\n",
    "        fgm = FGM(model)\n",
    "    if CFG.awp:\n",
    "        awp = AWP(model,\n",
    "                  optimizer, \n",
    "                  adv_lr = CFG.adv_lr, \n",
    "                  adv_eps = CFG.adv_eps, \n",
    "                  scaler = scaler)\n",
    "    for step, (inputs, labels) in enumerate(train_loader):\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled = CFG.apex):\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds, labels)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        if CFG.unscale:\n",
    "            scaler.unscale_(optimizer)\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        \n",
    "        #Fast Gradient Method (FGM)\n",
    "        if CFG.fgm:\n",
    "            fgm.attack()\n",
    "            with torch.cuda.amp.autocast(enabled = CFG.apex):\n",
    "                y_preds = model(inputs)\n",
    "                loss_adv = criterion(y_preds, labels)\n",
    "                loss_adv.backward()\n",
    "            fgm.restore()\n",
    "            \n",
    "        #Adversarial Weight Perturbation (AWP)\n",
    "        if CFG.awp:\n",
    "            loss_awp = awp.attack_backward(inputs, labels, attention_mask, step + 1)\n",
    "            loss_awp.backward()\n",
    "            awp._restore()\n",
    "        \n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f} '\n",
    "                  'LR: {lr:.8f} '\n",
    "                  .format(epoch + 1, step, len(train_loader), remain = timeSince(start, float(step + 1)/len(train_loader)),\n",
    "                          loss = losses,\n",
    "                          grad_norm = grad_norm,\n",
    "                          lr = scheduler.get_lr()[0]\n",
    "                         )\n",
    "                 )\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b2d1c",
   "metadata": {
    "id": "97cdf99c",
    "papermill": {
     "duration": 0.008297,
     "end_time": "2022-11-29T17:44:40.626161",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.617864",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Valid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c997d4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T17:44:40.644668Z",
     "iopub.status.busy": "2022-11-29T17:44:40.643941Z",
     "iopub.status.idle": "2022-11-29T17:44:40.652837Z",
     "shell.execute_reply": "2022-11-29T17:44:40.652005Z"
    },
    "id": "c593bba1",
    "papermill": {
     "duration": 0.020047,
     "end_time": "2022-11-29T17:44:40.654709",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.634662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, labels) in enumerate(valid_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds, labels)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader) - 1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss = losses,\n",
    "                          remain = timeSince(start, float(step + 1) / len(valid_loader))\n",
    "                         )\n",
    "                 )\n",
    "    return losses.avg, np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456755da",
   "metadata": {
    "id": "59104d7e",
    "papermill": {
     "duration": 0.008278,
     "end_time": "2022-11-29T17:44:40.671368",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.663090",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09a208f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T17:44:40.689937Z",
     "iopub.status.busy": "2022-11-29T17:44:40.689061Z",
     "iopub.status.idle": "2022-11-29T17:44:40.695678Z",
     "shell.execute_reply": "2022-11-29T17:44:40.694901Z"
    },
    "id": "672fd430",
    "outputId": "d253b3c9-8241-4038-af78-6673a89c6217",
    "papermill": {
     "duration": 0.017628,
     "end_time": "2022-11-29T17:44:40.697526",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.679898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.train:\n",
    "    LOGGER = get_logger()\n",
    "    LOGGER.info(f'OUTPUT_DIR: {CFG.OUTPUT_DIR}')\n",
    "    CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "    CFG.tokenizer.save_pretrained(CFG.OUTPUT_DIR + 'tokenizer')\n",
    "\n",
    "    #max_len\n",
    "    lengths = []\n",
    "    tk0 = tqdm(CFG.df_train['full_text'].fillna('').values, total = len(CFG.df_train))\n",
    "    for text in tk0:\n",
    "        length = len(CFG.tokenizer(text, add_special_tokens = False)['input_ids'])\n",
    "        lengths.append(length)\n",
    "    CFG.max_len = max(lengths) + 2\n",
    "    LOGGER.info(f'max_len: {CFG.max_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0752748",
   "metadata": {
    "id": "cdeaf267",
    "papermill": {
     "duration": 0.008383,
     "end_time": "2022-11-29T17:44:40.714367",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.705984",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f5237bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T17:44:40.732829Z",
     "iopub.status.busy": "2022-11-29T17:44:40.732043Z",
     "iopub.status.idle": "2022-11-29T17:44:40.738090Z",
     "shell.execute_reply": "2022-11-29T17:44:40.737329Z"
    },
    "id": "3ff61a44",
    "papermill": {
     "duration": 0.017171,
     "end_time": "2022-11-29T17:44:40.740030",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.722859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FB3TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['text'].values\n",
    "        self.labels = df[cfg.target_cols].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg, self.texts[item])\n",
    "        label = torch.tensor(self.labels[item], dtype = torch.float)\n",
    "        return inputs, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d9761b",
   "metadata": {
    "id": "a78dbf81",
    "papermill": {
     "duration": 0.008346,
     "end_time": "2022-11-29T17:44:40.756817",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.748471",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model\n",
    "\n",
    "* Initializing module (normal, xavier_uniform, xavier_normal, kaiming_uniform, kaiming_normal, orthogonal) \n",
    "* Freeze lower layer when you use large model (v2-xlarge, funnnel, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ba044d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T17:44:40.775076Z",
     "iopub.status.busy": "2022-11-29T17:44:40.774826Z",
     "iopub.status.idle": "2022-11-29T17:44:40.796547Z",
     "shell.execute_reply": "2022-11-29T17:44:40.795677Z"
    },
    "id": "0f57c98c",
    "papermill": {
     "duration": 0.033126,
     "end_time": "2022-11-29T17:44:40.798444",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.765318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FB3Model(nn.Module):\n",
    "    def __init__(self, CFG, config_path = None, pretrained = False):\n",
    "        super().__init__()\n",
    "        self.CFG = CFG\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(CFG.model, ouput_hidden_states = True)\n",
    "            self.config.save_pretrained(CFG.OUTPUT_DIR + 'config')\n",
    "            self.config.hidden_dropout = 0.\n",
    "            self.config.hidden_dropout_prob = 0.\n",
    "            self.config.attention_dropout = 0.\n",
    "            self.config.attention_probs_dropout_prob = 0.\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "            \n",
    "#         LOGGER.info(self.config)\n",
    "        \n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(CFG.model, config=self.config)\n",
    "            self.model.save_pretrained(CFG.OUTPUT_DIR + 'model')\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "            \n",
    "        if self.CFG.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "            \n",
    "        if CFG.pooling == 'mean':\n",
    "            self.pool = MeanPooling()\n",
    "        elif CFG.pooling == 'max':\n",
    "            self.pool = MaxPooling()\n",
    "        elif CFG.pooling == 'min':\n",
    "            self.pool = MinPooling()\n",
    "        elif CFG.pooling == 'attention':\n",
    "            self.pool = AttentionPooling(self.config.hidden_size)\n",
    "        elif CFG.pooling == 'weightedlayer':\n",
    "            self.pool = WeightedLayerPooling(self.config.num_hidden_layers, layer_start = CFG.layer_start, layer_weights = None)        \n",
    "        \n",
    "        self.fc = nn.Linear(self.config.hidden_size, self.CFG.n_targets)\n",
    "        self._init_weights(self.fc)\n",
    "        \n",
    "        if 'deberta-v2-xxlarge' in CFG.model:\n",
    "            self.model.embeddings.requires_grad_(False)\n",
    "            self.model.encoder.layer[:24].requires_grad_(False)\n",
    "        if 'deberta-v2-xlarge' in CFG.model:\n",
    "            self.model.embeddings.requires_grad_(False)\n",
    "            self.model.encoder.layer[:12].requires_grad_(False)\n",
    "        if 'funnel-transformer-xlarge' in CFG.model:\n",
    "            self.model.embeddings.requires_grad_(False)\n",
    "            self.model.encoder.blocks[:1].requires_grad_(False)\n",
    "        if 'funnel-transformer-large' in CFG.model:\n",
    "            self.model.embeddings.requires_grad_(False)\n",
    "            self.model.encoder.blocks[:1].requires_grad_(False)\n",
    "        if 'deberta-large' in CFG.model:\n",
    "            self.model.embeddings.requires_grad_(False)\n",
    "            self.model.encoder.layer[:16].requires_grad_(False)\n",
    "        if 'deberta-xlarge' in CFG.model:\n",
    "            self.model.embeddings.requires_grad_(False)\n",
    "            self.model.encoder.layer[:36].requires_grad_(False)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if CFG.init_weight == 'normal':\n",
    "                module.weight.data.normal_(mean = 0.0, std = self.config.initializer_range)\n",
    "            elif CFG.init_weight == 'xavier_uniform':\n",
    "                module.weight.data = nn.init.xavier_uniform_(module.weight.data)\n",
    "            elif CFG.init_weight == 'xavier_normal':\n",
    "                module.weight.data = nn.init.xavier_normal_(module.weight.data)\n",
    "            elif CFG.init_weight == 'kaiming_uniform':\n",
    "                module.weight.data = nn.init.kaiming_uniform_(module.weight.data)\n",
    "            elif CFG.init_weight == 'kaiming_normal':\n",
    "                module.weight.data = nn.init.kaiming_normal_(module.weight.data)\n",
    "            elif CFG.init_weight == 'orthogonal':\n",
    "                module.weight.data = nn.init.orthogonal_(module.weight.data)\n",
    "                \n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            if CFG.init_weight == 'normal':\n",
    "                module.weight.data.normal_(mean = 0.0, std = self.config.initializer_range)\n",
    "            elif CFG.init_weight == 'xavier_uniform':\n",
    "                module.weight.data = nn.init.xavier_uniform_(module.weight.data)\n",
    "            elif CFG.init_weight == 'xavier_normal':\n",
    "                module.weight.data = nn.init.xavier_normal_(module.weight.data)\n",
    "            elif CFG.init_weight == 'kaiming_uniform':\n",
    "                module.weight.data = nn.init.kaiming_uniform_(module.weight.data)\n",
    "            elif CFG.init_weight == 'kaiming_normal':\n",
    "                module.weight.data = nn.init.kaiming_normal_(module.weight.data)\n",
    "            elif CFG.init_weight == 'orthogonal':\n",
    "                module.weight.data = nn.init.orthogonal_(module.weight.data)\n",
    "                \n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        if CFG.pooling != 'weightedlayer':\n",
    "            last_hidden_states = outputs[0]\n",
    "            feature = self.pool(last_hidden_states, inputs['attention_mask'])\n",
    "        else:\n",
    "            all_layer_embeddings = outputs[1]\n",
    "            feature = self.pool(all_layer_embeddings)\n",
    "            \n",
    "        return feature\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        outout = self.fc(feature)\n",
    "        return outout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de0b347",
   "metadata": {
    "id": "37df1158",
    "papermill": {
     "duration": 0.00867,
     "end_time": "2022-11-29T17:44:40.815792",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.807122",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train\n",
    "* Re-initializing upper layer (normal, xavier_uniform, xavier_normal, kaiming_uniform, kaiming_normal, orthogonal) \n",
    "* Layer-Wise Learning Rate Dacay (https://www.kaggle.com/code/rhtsingh/on-stability-of-few-sample-transformer-fine-tuning?scriptVersionId=67176591&cellId=29)\n",
    "* Loss function, SmoothL1 or RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6ea9768",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T17:44:40.835701Z",
     "iopub.status.busy": "2022-11-29T17:44:40.835423Z",
     "iopub.status.idle": "2022-11-29T17:44:40.869622Z",
     "shell.execute_reply": "2022-11-29T17:44:40.868615Z"
    },
    "id": "dd13fa6f",
    "papermill": {
     "duration": 0.046995,
     "end_time": "2022-11-29T17:44:40.871692",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.824697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def re_initializing_layer(model, config, layer_num):\n",
    "    for module in model.model.encoder.layer[-layer_num:].modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if CFG.init_weight == 'normal':\n",
    "                module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
    "            elif CFG.init_weight == 'xavier_uniform':\n",
    "                module.weight.data = nn.init.xavier_uniform_(module.weight.data)\n",
    "            elif CFG.init_weight == 'xavier_normal':\n",
    "                module.weight.data = nn.init.xavier_normal_(module.weight.data)\n",
    "            elif CFG.init_weight == 'kaiming_uniform':\n",
    "                module.weight.data = nn.init.kaiming_uniform_(module.weight.data)\n",
    "            elif CFG.init_weight == 'kaiming_normal':\n",
    "                module.weight.data = nn.init.kaiming_normal_(module.weight.data)\n",
    "            elif CFG.init_weight == 'orthogonal':\n",
    "                module.weight.data = nn.init.orthogonal_(module.weight.data) \n",
    "                \n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            if CFG.init_weight == 'normal':\n",
    "                module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
    "            elif CFG.init_weight == 'xavier_uniform':\n",
    "                module.weight.data = nn.init.xavier_uniform_(module.weight.data)\n",
    "            elif CFG.init_weight == 'xavier_normal':\n",
    "                module.weight.data = nn.init.xavier_normal_(module.weight.data)\n",
    "            elif CFG.init_weight == 'kaiming_uniform':\n",
    "                module.weight.data = nn.init.kaiming_uniform_(module.weight.data)\n",
    "            elif CFG.init_weight == 'kaiming_normal':\n",
    "                module.weight.data = nn.init.kaiming_normal_(module.weight.data)\n",
    "            elif CFG.init_weight == 'orthogonal':\n",
    "                module.weight.data = nn.init.orthogonal_(module.weight.data)\n",
    "                \n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    return model   \n",
    "\n",
    "def train_loop(folds, fold):\n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "    \n",
    "    train_folds = folds[folds['fold'] != fold].reset_index(drop = True)\n",
    "    valid_folds = folds[folds['fold'] == fold].reset_index(drop = True)\n",
    "    valid_labels = valid_folds[CFG.target_cols].values\n",
    "    \n",
    "    train_dataset = FB3TrainDataset(CFG, train_folds)\n",
    "    valid_dataset = FB3TrainDataset(CFG, valid_folds)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size = CFG.batch_size,\n",
    "                              shuffle = True, \n",
    "                              num_workers = CFG.num_workers,\n",
    "                              pin_memory = True, \n",
    "                              drop_last = True\n",
    "                             )\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size = CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers,\n",
    "                              pin_memory=True, \n",
    "                              drop_last=False)\n",
    "\n",
    "    model = FB3Model(CFG, config_path = None, pretrained = True)\n",
    "    if CFG.reinit:\n",
    "        model = re_initializing_layer(model, model.config, CFG.reinit_n)\n",
    "        \n",
    "    #os.makedirs(CFG.OUTPUT_DIR + 'config/', exist_ok = True)\n",
    "    #torch.save(model.config, CFG.OUTPUT_DIR + 'config/config.pth')\n",
    "    model.to(CFG.device)\n",
    "    \n",
    "    def get_optimizer_params(model,\n",
    "                             encoder_lr,\n",
    "                             decoder_lr,\n",
    "                             weight_decay=0.0):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr,\n",
    "             'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr,\n",
    "             'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "             'lr': decoder_lr,\n",
    "             'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "    \n",
    "    #llrd\n",
    "    def get_optimizer_grouped_parameters(model, \n",
    "                                         layerwise_lr,\n",
    "                                         layerwise_weight_decay,\n",
    "                                         layerwise_lr_decay):\n",
    "        \n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        # initialize lr for task specific layer\n",
    "        optimizer_grouped_parameters = [{\"params\": [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "                                         \"weight_decay\": 0.0,\n",
    "                                         \"lr\": layerwise_lr,\n",
    "                                        },]\n",
    "        # initialize lrs for every layer\n",
    "        layers = [model.model.embeddings] + list(model.model.encoder.layer)\n",
    "        layers.reverse()\n",
    "        lr = layerwise_lr\n",
    "        for layer in layers:\n",
    "            optimizer_grouped_parameters += [{\"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                                              \"weight_decay\": layerwise_weight_decay,\n",
    "                                              \"lr\": lr,\n",
    "                                             },\n",
    "                                             {\"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                                              \"weight_decay\": 0.0,\n",
    "                                              \"lr\": lr,\n",
    "                                             },]\n",
    "            lr *= layerwise_lr_decay\n",
    "        return optimizer_grouped_parameters\n",
    "    \n",
    "    if CFG.llrd:\n",
    "        from transformers import AdamW\n",
    "        grouped_optimizer_params = get_optimizer_grouped_parameters(model, \n",
    "                                                                    CFG.layerwise_lr, \n",
    "                                                                    CFG.layerwise_weight_decay, \n",
    "                                                                    CFG.layerwise_lr_decay)\n",
    "        optimizer = AdamW(grouped_optimizer_params,\n",
    "                          lr = CFG.layerwise_lr,\n",
    "                          eps = CFG.layerwise_adam_epsilon,\n",
    "                          correct_bias = not CFG.layerwise_use_bertadam)\n",
    "    else:\n",
    "        from torch.optim import AdamW\n",
    "        optimizer_parameters = get_optimizer_params(model,\n",
    "                                                    encoder_lr=CFG.encoder_lr, \n",
    "                                                    decoder_lr=CFG.decoder_lr,\n",
    "                                                    weight_decay=CFG.weight_decay)\n",
    "        optimizer = AdamW(optimizer_parameters, \n",
    "                          lr=CFG.encoder_lr,\n",
    "                          eps=CFG.eps,\n",
    "                          betas=CFG.betas)\n",
    "    \n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        if cfg.scheduler == 'linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, \n",
    "                num_warmup_steps = cfg.num_warmup_steps, \n",
    "                num_training_steps = num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler == 'cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, \n",
    "                num_warmup_steps = cfg.num_warmup_steps, \n",
    "                num_training_steps = num_train_steps,\n",
    "                num_cycles = cfg.num_cycles\n",
    "            )\n",
    "        return scheduler\n",
    "    \n",
    "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "    \n",
    "    if CFG.loss_func == 'SmoothL1':\n",
    "        criterion = nn.SmoothL1Loss(reduction='mean')\n",
    "    elif CFG.loss_func == 'RMSE':\n",
    "        criterion = RMSELoss(reduction='mean')\n",
    "    \n",
    "    best_score = np.inf\n",
    "    best_train_loss = np.inf\n",
    "    best_val_loss = np.inf\n",
    "    \n",
    "    epoch_list = []\n",
    "    epoch_avg_loss_list = []\n",
    "    epoch_avg_val_loss_list = []\n",
    "    epoch_score_list = []\n",
    "    epoch_scores_list = []\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, CFG.device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, CFG.device)\n",
    "        \n",
    "        # scoring\n",
    "        score, scores = get_score(valid_labels, predictions)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {scores}')\n",
    "        \n",
    "        epoch_list.append(epoch+1)\n",
    "        epoch_avg_loss_list.append(avg_loss)\n",
    "        epoch_avg_val_loss_list.append(avg_val_loss)\n",
    "        epoch_score_list.append(score)\n",
    "        epoch_scores_list.append(scores)\n",
    "        \n",
    "        if best_score > score:\n",
    "            best_score = score\n",
    "            best_train_loss = avg_loss\n",
    "            best_val_loss = avg_val_loss\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                        CFG.OUTPUT_DIR + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n",
    "            \n",
    "        if CFG.save_all_models:\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                        CFG.OUTPUT_DIR + f\"{CFG.model.replace('/', '-')}_fold{fold}_epoch{epoch + 1}.pth\")\n",
    "\n",
    "    predictions = torch.load(CFG.OUTPUT_DIR + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n",
    "                             map_location = torch.device('cpu'))['predictions']\n",
    "    valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n",
    "    \n",
    "    df_epoch = pd.DataFrame({'epoch' : epoch_list,\n",
    "                             'MCRMSE' : epoch_score_list,\n",
    "                             'train_loss' : epoch_avg_loss_list, \n",
    "                             'val_loss' : epoch_avg_val_loss_list})\n",
    "    df_scores = pd.DataFrame(epoch_scores_list)\n",
    "    df_scores.columns = CFG.target_cols\n",
    "    \n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return best_train_loss, best_val_loss, valid_folds, pd.concat([df_epoch, df_scores], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb2dd01",
   "metadata": {
    "id": "1880b509",
    "papermill": {
     "duration": 0.008346,
     "end_time": "2022-11-29T17:44:40.888717",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.880371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Run !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bae471f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T17:44:40.907114Z",
     "iopub.status.busy": "2022-11-29T17:44:40.906859Z",
     "iopub.status.idle": "2022-11-29T17:44:40.917856Z",
     "shell.execute_reply": "2022-11-29T17:44:40.917055Z"
    },
    "id": "68c85b19",
    "outputId": "77d93ad6-3c13-4089-cdaa-4d86da94d97f",
    "papermill": {
     "duration": 0.02267,
     "end_time": "2022-11-29T17:44:40.919883",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.897213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_result(oof_df, fold, best_train_loss, best_val_loss):\n",
    "    labels = oof_df[CFG.target_cols].values\n",
    "    preds = oof_df[[f\"pred_{c}\" for c in CFG.target_cols]].values\n",
    "    score, scores = get_score(labels, preds)\n",
    "    LOGGER.info(f'Score: {score:<.4f}  Scores: {scores}')\n",
    "    _output_log = pd.DataFrame([CFG.identifier, CFG.model, CFG.cv_seed, CFG.seed, fold, 'best', score, best_train_loss, best_val_loss] + scores).T\n",
    "    _output_log.columns = ['file', 'model', 'cv_seed', 'seed', 'fold', 'epoch', 'MCRMSE', 'train_loss', 'val_loss'] + CFG.target_cols\n",
    "    return _output_log\n",
    "\n",
    "if CFG.train:\n",
    "    output_log = pd.DataFrame()\n",
    "    oof_df = pd.DataFrame()\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    for fold in range(CFG.n_fold):\n",
    "        if fold in CFG.trn_fold:\n",
    "            best_train_loss, best_val_loss, _oof_df, df_epoch_scores = train_loop(CFG.df_train, fold)\n",
    "            train_loss_list.append(best_train_loss)\n",
    "            val_loss_list.append(best_val_loss)\n",
    "            oof_df = pd.concat([oof_df, _oof_df])\n",
    "            LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "\n",
    "            df_epoch_scores['file'] = CFG.identifier\n",
    "            df_epoch_scores['model'] = CFG.model\n",
    "            df_epoch_scores['cv_seed'] = CFG.cv_seed\n",
    "            df_epoch_scores['seed'] = CFG.seed\n",
    "            df_epoch_scores['fold'] = fold\n",
    "            df_epoch_scores = df_epoch_scores[['file', 'model', 'cv_seed', 'seed', 'fold', 'epoch', 'MCRMSE', 'train_loss', 'val_loss'] + CFG.target_cols]\n",
    "\n",
    "            _output_log = get_result(_oof_df, fold, best_train_loss, best_val_loss)\n",
    "            output_log = pd.concat([output_log, df_epoch_scores, _output_log])\n",
    "\n",
    "    oof_df = oof_df.reset_index(drop=True)\n",
    "    LOGGER.info(f\"========== CV ==========\")\n",
    "    _output_log = get_result(oof_df, 'OOF', np.mean(train_loss_list), np.mean(val_loss_list))\n",
    "    output_log = pd.concat([output_log, _output_log])\n",
    "    output_log.to_csv(f'{CFG.identifier}.csv', index=False)\n",
    "    oof_df.to_pickle(CFG.OUTPUT_DIR+'oof_df.pkl', protocol = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cfd08a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T17:44:40.938233Z",
     "iopub.status.busy": "2022-11-29T17:44:40.937647Z",
     "iopub.status.idle": "2022-11-29T17:44:40.979445Z",
     "shell.execute_reply": "2022-11-29T17:44:40.978347Z"
    },
    "id": "p0gF5LrWA0vk",
    "papermill": {
     "duration": 0.054075,
     "end_time": "2022-11-29T17:44:40.982400",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.928325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>model</th>\n",
       "      <th>cv_seed</th>\n",
       "      <th>seed</th>\n",
       "      <th>fold</th>\n",
       "      <th>epoch</th>\n",
       "      <th>MCRMSE</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.561000</td>\n",
       "      <td>0.521126</td>\n",
       "      <td>0.449782</td>\n",
       "      <td>0.516851</td>\n",
       "      <td>0.475575</td>\n",
       "      <td>0.527028</td>\n",
       "      <td>0.636209</td>\n",
       "      <td>0.686026</td>\n",
       "      <td>0.524309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.488077</td>\n",
       "      <td>0.389696</td>\n",
       "      <td>0.389838</td>\n",
       "      <td>0.507989</td>\n",
       "      <td>0.490968</td>\n",
       "      <td>0.438906</td>\n",
       "      <td>0.489199</td>\n",
       "      <td>0.497672</td>\n",
       "      <td>0.503726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.474499</td>\n",
       "      <td>0.371241</td>\n",
       "      <td>0.378357</td>\n",
       "      <td>0.501092</td>\n",
       "      <td>0.457945</td>\n",
       "      <td>0.429419</td>\n",
       "      <td>0.491952</td>\n",
       "      <td>0.498496</td>\n",
       "      <td>0.468088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.469976</td>\n",
       "      <td>0.357196</td>\n",
       "      <td>0.375450</td>\n",
       "      <td>0.497641</td>\n",
       "      <td>0.453045</td>\n",
       "      <td>0.424740</td>\n",
       "      <td>0.482539</td>\n",
       "      <td>0.502159</td>\n",
       "      <td>0.459731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.465933</td>\n",
       "      <td>0.347559</td>\n",
       "      <td>0.371985</td>\n",
       "      <td>0.497340</td>\n",
       "      <td>0.454148</td>\n",
       "      <td>0.424571</td>\n",
       "      <td>0.475527</td>\n",
       "      <td>0.485295</td>\n",
       "      <td>0.458720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>0</td>\n",
       "      <td>best</td>\n",
       "      <td>0.465933</td>\n",
       "      <td>0.347559</td>\n",
       "      <td>0.371985</td>\n",
       "      <td>0.497340</td>\n",
       "      <td>0.454148</td>\n",
       "      <td>0.424571</td>\n",
       "      <td>0.475527</td>\n",
       "      <td>0.485295</td>\n",
       "      <td>0.458720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.501910</td>\n",
       "      <td>0.522812</td>\n",
       "      <td>0.400567</td>\n",
       "      <td>0.608535</td>\n",
       "      <td>0.494503</td>\n",
       "      <td>0.430736</td>\n",
       "      <td>0.467862</td>\n",
       "      <td>0.504015</td>\n",
       "      <td>0.505809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.479081</td>\n",
       "      <td>0.390469</td>\n",
       "      <td>0.386325</td>\n",
       "      <td>0.505927</td>\n",
       "      <td>0.481451</td>\n",
       "      <td>0.474206</td>\n",
       "      <td>0.459750</td>\n",
       "      <td>0.490512</td>\n",
       "      <td>0.462639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.492044</td>\n",
       "      <td>0.372912</td>\n",
       "      <td>0.394243</td>\n",
       "      <td>0.565600</td>\n",
       "      <td>0.469719</td>\n",
       "      <td>0.424702</td>\n",
       "      <td>0.488875</td>\n",
       "      <td>0.477427</td>\n",
       "      <td>0.525941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.459839</td>\n",
       "      <td>0.356963</td>\n",
       "      <td>0.368560</td>\n",
       "      <td>0.501356</td>\n",
       "      <td>0.447263</td>\n",
       "      <td>0.415428</td>\n",
       "      <td>0.463078</td>\n",
       "      <td>0.473729</td>\n",
       "      <td>0.458179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.457040</td>\n",
       "      <td>0.348684</td>\n",
       "      <td>0.365823</td>\n",
       "      <td>0.497288</td>\n",
       "      <td>0.446088</td>\n",
       "      <td>0.411648</td>\n",
       "      <td>0.458758</td>\n",
       "      <td>0.473040</td>\n",
       "      <td>0.455419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>best</td>\n",
       "      <td>0.457040</td>\n",
       "      <td>0.348684</td>\n",
       "      <td>0.365823</td>\n",
       "      <td>0.497288</td>\n",
       "      <td>0.446088</td>\n",
       "      <td>0.411648</td>\n",
       "      <td>0.458758</td>\n",
       "      <td>0.473040</td>\n",
       "      <td>0.455419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.511666</td>\n",
       "      <td>0.528458</td>\n",
       "      <td>0.409185</td>\n",
       "      <td>0.566715</td>\n",
       "      <td>0.450833</td>\n",
       "      <td>0.482897</td>\n",
       "      <td>0.466642</td>\n",
       "      <td>0.600624</td>\n",
       "      <td>0.502285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.462474</td>\n",
       "      <td>0.398653</td>\n",
       "      <td>0.368378</td>\n",
       "      <td>0.499631</td>\n",
       "      <td>0.438414</td>\n",
       "      <td>0.439412</td>\n",
       "      <td>0.470398</td>\n",
       "      <td>0.475254</td>\n",
       "      <td>0.451735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.457054</td>\n",
       "      <td>0.377067</td>\n",
       "      <td>0.365659</td>\n",
       "      <td>0.492475</td>\n",
       "      <td>0.435449</td>\n",
       "      <td>0.422033</td>\n",
       "      <td>0.464239</td>\n",
       "      <td>0.475962</td>\n",
       "      <td>0.452165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.458266</td>\n",
       "      <td>0.363570</td>\n",
       "      <td>0.365715</td>\n",
       "      <td>0.470067</td>\n",
       "      <td>0.447306</td>\n",
       "      <td>0.426129</td>\n",
       "      <td>0.478372</td>\n",
       "      <td>0.472122</td>\n",
       "      <td>0.455602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.444468</td>\n",
       "      <td>0.353449</td>\n",
       "      <td>0.354477</td>\n",
       "      <td>0.463118</td>\n",
       "      <td>0.431336</td>\n",
       "      <td>0.413687</td>\n",
       "      <td>0.447777</td>\n",
       "      <td>0.463204</td>\n",
       "      <td>0.447686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>best</td>\n",
       "      <td>0.444468</td>\n",
       "      <td>0.353449</td>\n",
       "      <td>0.354477</td>\n",
       "      <td>0.463118</td>\n",
       "      <td>0.431336</td>\n",
       "      <td>0.413687</td>\n",
       "      <td>0.447777</td>\n",
       "      <td>0.463204</td>\n",
       "      <td>0.447686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.567570</td>\n",
       "      <td>0.522189</td>\n",
       "      <td>0.457913</td>\n",
       "      <td>0.514981</td>\n",
       "      <td>0.619547</td>\n",
       "      <td>0.449535</td>\n",
       "      <td>0.532113</td>\n",
       "      <td>0.711107</td>\n",
       "      <td>0.578138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.524735</td>\n",
       "      <td>0.395136</td>\n",
       "      <td>0.419742</td>\n",
       "      <td>0.589243</td>\n",
       "      <td>0.508732</td>\n",
       "      <td>0.479392</td>\n",
       "      <td>0.501746</td>\n",
       "      <td>0.539863</td>\n",
       "      <td>0.529432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.493496</td>\n",
       "      <td>0.371153</td>\n",
       "      <td>0.396879</td>\n",
       "      <td>0.504848</td>\n",
       "      <td>0.494092</td>\n",
       "      <td>0.463824</td>\n",
       "      <td>0.480871</td>\n",
       "      <td>0.547168</td>\n",
       "      <td>0.470172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.482734</td>\n",
       "      <td>0.357669</td>\n",
       "      <td>0.386583</td>\n",
       "      <td>0.510883</td>\n",
       "      <td>0.490725</td>\n",
       "      <td>0.433049</td>\n",
       "      <td>0.482061</td>\n",
       "      <td>0.514695</td>\n",
       "      <td>0.464993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.473116</td>\n",
       "      <td>0.347526</td>\n",
       "      <td>0.379425</td>\n",
       "      <td>0.502947</td>\n",
       "      <td>0.463305</td>\n",
       "      <td>0.431576</td>\n",
       "      <td>0.466980</td>\n",
       "      <td>0.511003</td>\n",
       "      <td>0.462888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>best</td>\n",
       "      <td>0.473116</td>\n",
       "      <td>0.347526</td>\n",
       "      <td>0.379425</td>\n",
       "      <td>0.502947</td>\n",
       "      <td>0.463305</td>\n",
       "      <td>0.431576</td>\n",
       "      <td>0.466980</td>\n",
       "      <td>0.511003</td>\n",
       "      <td>0.462888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.515704</td>\n",
       "      <td>0.519234</td>\n",
       "      <td>0.412506</td>\n",
       "      <td>0.522299</td>\n",
       "      <td>0.567910</td>\n",
       "      <td>0.467501</td>\n",
       "      <td>0.519862</td>\n",
       "      <td>0.510280</td>\n",
       "      <td>0.506373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.468767</td>\n",
       "      <td>0.397333</td>\n",
       "      <td>0.374415</td>\n",
       "      <td>0.519684</td>\n",
       "      <td>0.465943</td>\n",
       "      <td>0.432585</td>\n",
       "      <td>0.463231</td>\n",
       "      <td>0.473962</td>\n",
       "      <td>0.457197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.475368</td>\n",
       "      <td>0.375294</td>\n",
       "      <td>0.380028</td>\n",
       "      <td>0.527519</td>\n",
       "      <td>0.467361</td>\n",
       "      <td>0.449510</td>\n",
       "      <td>0.454887</td>\n",
       "      <td>0.470238</td>\n",
       "      <td>0.482692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.458091</td>\n",
       "      <td>0.357467</td>\n",
       "      <td>0.365983</td>\n",
       "      <td>0.491698</td>\n",
       "      <td>0.460752</td>\n",
       "      <td>0.426437</td>\n",
       "      <td>0.453299</td>\n",
       "      <td>0.465610</td>\n",
       "      <td>0.450748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.457523</td>\n",
       "      <td>0.350602</td>\n",
       "      <td>0.364886</td>\n",
       "      <td>0.492002</td>\n",
       "      <td>0.461462</td>\n",
       "      <td>0.424770</td>\n",
       "      <td>0.452015</td>\n",
       "      <td>0.464851</td>\n",
       "      <td>0.450038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>best</td>\n",
       "      <td>0.457523</td>\n",
       "      <td>0.350602</td>\n",
       "      <td>0.364886</td>\n",
       "      <td>0.492002</td>\n",
       "      <td>0.461462</td>\n",
       "      <td>0.424770</td>\n",
       "      <td>0.452015</td>\n",
       "      <td>0.464851</td>\n",
       "      <td>0.450038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>20221129-081736-/kaggle/input/pretran-model/pr...</td>\n",
       "      <td>/kaggle/input/pretran-model/pretrained_models/...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>OOF</td>\n",
       "      <td>best</td>\n",
       "      <td>0.459780</td>\n",
       "      <td>0.349564</td>\n",
       "      <td>0.367319</td>\n",
       "      <td>0.490791</td>\n",
       "      <td>0.451411</td>\n",
       "      <td>0.421296</td>\n",
       "      <td>0.460364</td>\n",
       "      <td>0.479813</td>\n",
       "      <td>0.455002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 file  \\\n",
       "0   20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "1   20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "2   20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "3   20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "4   20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "5   20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "6   20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "7   20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "8   20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "9   20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "10  20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "11  20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "12  20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "13  20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "14  20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "15  20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "16  20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "17  20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "18  20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "19  20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "20  20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "21  20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "22  20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "23  20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "24  20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "25  20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "26  20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "27  20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "28  20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "29  20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "30  20221129-081736-/kaggle/input/pretran-model/pr...   \n",
       "\n",
       "                                                model  cv_seed  seed fold  \\\n",
       "0   /kaggle/input/pretran-model/pretrained_models/...     2022  2022    0   \n",
       "1   /kaggle/input/pretran-model/pretrained_models/...     2022  2022    0   \n",
       "2   /kaggle/input/pretran-model/pretrained_models/...     2022  2022    0   \n",
       "3   /kaggle/input/pretran-model/pretrained_models/...     2022  2022    0   \n",
       "4   /kaggle/input/pretran-model/pretrained_models/...     2022  2022    0   \n",
       "5   /kaggle/input/pretran-model/pretrained_models/...     2022  2022    0   \n",
       "6   /kaggle/input/pretran-model/pretrained_models/...     2022  2022    1   \n",
       "7   /kaggle/input/pretran-model/pretrained_models/...     2022  2022    1   \n",
       "8   /kaggle/input/pretran-model/pretrained_models/...     2022  2022    1   \n",
       "9   /kaggle/input/pretran-model/pretrained_models/...     2022  2022    1   \n",
       "10  /kaggle/input/pretran-model/pretrained_models/...     2022  2022    1   \n",
       "11  /kaggle/input/pretran-model/pretrained_models/...     2022  2022    1   \n",
       "12  /kaggle/input/pretran-model/pretrained_models/...     2022  2022    2   \n",
       "13  /kaggle/input/pretran-model/pretrained_models/...     2022  2022    2   \n",
       "14  /kaggle/input/pretran-model/pretrained_models/...     2022  2022    2   \n",
       "15  /kaggle/input/pretran-model/pretrained_models/...     2022  2022    2   \n",
       "16  /kaggle/input/pretran-model/pretrained_models/...     2022  2022    2   \n",
       "17  /kaggle/input/pretran-model/pretrained_models/...     2022  2022    2   \n",
       "18  /kaggle/input/pretran-model/pretrained_models/...     2022  2022    3   \n",
       "19  /kaggle/input/pretran-model/pretrained_models/...     2022  2022    3   \n",
       "20  /kaggle/input/pretran-model/pretrained_models/...     2022  2022    3   \n",
       "21  /kaggle/input/pretran-model/pretrained_models/...     2022  2022    3   \n",
       "22  /kaggle/input/pretran-model/pretrained_models/...     2022  2022    3   \n",
       "23  /kaggle/input/pretran-model/pretrained_models/...     2022  2022    3   \n",
       "24  /kaggle/input/pretran-model/pretrained_models/...     2022  2022    4   \n",
       "25  /kaggle/input/pretran-model/pretrained_models/...     2022  2022    4   \n",
       "26  /kaggle/input/pretran-model/pretrained_models/...     2022  2022    4   \n",
       "27  /kaggle/input/pretran-model/pretrained_models/...     2022  2022    4   \n",
       "28  /kaggle/input/pretran-model/pretrained_models/...     2022  2022    4   \n",
       "29  /kaggle/input/pretran-model/pretrained_models/...     2022  2022    4   \n",
       "30  /kaggle/input/pretran-model/pretrained_models/...     2022  2022  OOF   \n",
       "\n",
       "   epoch    MCRMSE  train_loss  val_loss  cohesion    syntax  vocabulary  \\\n",
       "0      1  0.561000    0.521126  0.449782  0.516851  0.475575    0.527028   \n",
       "1      2  0.488077    0.389696  0.389838  0.507989  0.490968    0.438906   \n",
       "2      3  0.474499    0.371241  0.378357  0.501092  0.457945    0.429419   \n",
       "3      4  0.469976    0.357196  0.375450  0.497641  0.453045    0.424740   \n",
       "4      5  0.465933    0.347559  0.371985  0.497340  0.454148    0.424571   \n",
       "5   best  0.465933    0.347559  0.371985  0.497340  0.454148    0.424571   \n",
       "6      1  0.501910    0.522812  0.400567  0.608535  0.494503    0.430736   \n",
       "7      2  0.479081    0.390469  0.386325  0.505927  0.481451    0.474206   \n",
       "8      3  0.492044    0.372912  0.394243  0.565600  0.469719    0.424702   \n",
       "9      4  0.459839    0.356963  0.368560  0.501356  0.447263    0.415428   \n",
       "10     5  0.457040    0.348684  0.365823  0.497288  0.446088    0.411648   \n",
       "11  best  0.457040    0.348684  0.365823  0.497288  0.446088    0.411648   \n",
       "12     1  0.511666    0.528458  0.409185  0.566715  0.450833    0.482897   \n",
       "13     2  0.462474    0.398653  0.368378  0.499631  0.438414    0.439412   \n",
       "14     3  0.457054    0.377067  0.365659  0.492475  0.435449    0.422033   \n",
       "15     4  0.458266    0.363570  0.365715  0.470067  0.447306    0.426129   \n",
       "16     5  0.444468    0.353449  0.354477  0.463118  0.431336    0.413687   \n",
       "17  best  0.444468    0.353449  0.354477  0.463118  0.431336    0.413687   \n",
       "18     1  0.567570    0.522189  0.457913  0.514981  0.619547    0.449535   \n",
       "19     2  0.524735    0.395136  0.419742  0.589243  0.508732    0.479392   \n",
       "20     3  0.493496    0.371153  0.396879  0.504848  0.494092    0.463824   \n",
       "21     4  0.482734    0.357669  0.386583  0.510883  0.490725    0.433049   \n",
       "22     5  0.473116    0.347526  0.379425  0.502947  0.463305    0.431576   \n",
       "23  best  0.473116    0.347526  0.379425  0.502947  0.463305    0.431576   \n",
       "24     1  0.515704    0.519234  0.412506  0.522299  0.567910    0.467501   \n",
       "25     2  0.468767    0.397333  0.374415  0.519684  0.465943    0.432585   \n",
       "26     3  0.475368    0.375294  0.380028  0.527519  0.467361    0.449510   \n",
       "27     4  0.458091    0.357467  0.365983  0.491698  0.460752    0.426437   \n",
       "28     5  0.457523    0.350602  0.364886  0.492002  0.461462    0.424770   \n",
       "29  best  0.457523    0.350602  0.364886  0.492002  0.461462    0.424770   \n",
       "30  best  0.459780    0.349564  0.367319  0.490791  0.451411    0.421296   \n",
       "\n",
       "    phraseology   grammar  conventions  \n",
       "0      0.636209  0.686026     0.524309  \n",
       "1      0.489199  0.497672     0.503726  \n",
       "2      0.491952  0.498496     0.468088  \n",
       "3      0.482539  0.502159     0.459731  \n",
       "4      0.475527  0.485295     0.458720  \n",
       "5      0.475527  0.485295     0.458720  \n",
       "6      0.467862  0.504015     0.505809  \n",
       "7      0.459750  0.490512     0.462639  \n",
       "8      0.488875  0.477427     0.525941  \n",
       "9      0.463078  0.473729     0.458179  \n",
       "10     0.458758  0.473040     0.455419  \n",
       "11     0.458758  0.473040     0.455419  \n",
       "12     0.466642  0.600624     0.502285  \n",
       "13     0.470398  0.475254     0.451735  \n",
       "14     0.464239  0.475962     0.452165  \n",
       "15     0.478372  0.472122     0.455602  \n",
       "16     0.447777  0.463204     0.447686  \n",
       "17     0.447777  0.463204     0.447686  \n",
       "18     0.532113  0.711107     0.578138  \n",
       "19     0.501746  0.539863     0.529432  \n",
       "20     0.480871  0.547168     0.470172  \n",
       "21     0.482061  0.514695     0.464993  \n",
       "22     0.466980  0.511003     0.462888  \n",
       "23     0.466980  0.511003     0.462888  \n",
       "24     0.519862  0.510280     0.506373  \n",
       "25     0.463231  0.473962     0.457197  \n",
       "26     0.454887  0.470238     0.482692  \n",
       "27     0.453299  0.465610     0.450748  \n",
       "28     0.452015  0.464851     0.450038  \n",
       "29     0.452015  0.464851     0.450038  \n",
       "30     0.460364  0.479813     0.455002  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out=pd.read_csv(f\"/{CFG.identifier}.csv\")\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6397904c",
   "metadata": {
    "papermill": {
     "duration": 0.00905,
     "end_time": "2022-11-29T17:44:41.001069",
     "exception": false,
     "start_time": "2022-11-29T17:44:40.992019",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b54d01fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T17:44:41.020727Z",
     "iopub.status.busy": "2022-11-29T17:44:41.020206Z",
     "iopub.status.idle": "2022-11-29T17:44:41.042434Z",
     "shell.execute_reply": "2022-11-29T17:44:41.041620Z"
    },
    "papermill": {
     "duration": 0.034202,
     "end_time": "2022-11-29T17:44:41.044366",
     "exception": false,
     "start_time": "2022-11-29T17:44:41.010164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa54763f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T17:44:41.064073Z",
     "iopub.status.busy": "2022-11-29T17:44:41.063279Z",
     "iopub.status.idle": "2022-11-29T17:44:41.401220Z",
     "shell.execute_reply": "2022-11-29T17:44:41.400300Z"
    },
    "papermill": {
     "duration": 0.350035,
     "end_time": "2022-11-29T17:44:41.403527",
     "exception": false,
     "start_time": "2022-11-29T17:44:41.053492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/fnetune/20221129-081736-/kaggle/input/pretran-model/pretrained_models/microsoft/deberta-v3-base1\n",
      "### /kaggle/input/fnetune/20221129-081736-/kaggle/input/pretran-model/pretrained_models/microsoft/deberta-v3-base1/tokenizer\n"
     ]
    }
   ],
   "source": [
    "class TESTCFG:\n",
    "    num_workers=4\n",
    "    path=f\"/{CFG.identifier}\"\n",
    "    print(path)\n",
    "    config_path=f\"{path}/config/config.json\"\n",
    "    model=\"-kaggle-input-pretran-model-pretrained_models-microsoft-deberta-v3-base1\"\n",
    "    print(\"###\",f\"{path}/tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(f\"{path}/tokenizer\")\n",
    "    gradient_checkpointing=False\n",
    "    batch_size = 24#4\n",
    "    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    seed=42\n",
    "    n_fold=3\n",
    "    trn_fold=list(range(n_fold))\n",
    "    pooling = 'attention'\n",
    "    layer_start = 4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52eb3d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T17:44:41.425549Z",
     "iopub.status.busy": "2022-11-29T17:44:41.423885Z",
     "iopub.status.idle": "2022-11-29T17:44:41.577440Z",
     "shell.execute_reply": "2022-11-29T17:44:41.575888Z"
    },
    "papermill": {
     "duration": 0.166968,
     "end_time": "2022-11-29T17:44:41.580404",
     "exception": false,
     "start_time": "2022-11-29T17:44:41.413436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.4598  Scores: [0.49079104318558286, 0.45141084629114714, 0.4212963858753465, 0.460364149500473, 0.47981291551287875, 0.45500169634759097]\n"
     ]
    }
   ],
   "source": [
    "oof_df = pd.read_pickle(TESTCFG.path + '/oof_df.pkl')\n",
    "labels = oof_df[TESTCFG.target_cols].values\n",
    "preds = oof_df[[f\"pred_{c}\" for c in TESTCFG.target_cols]].values\n",
    "score, scores = get_score(labels, preds)\n",
    "print(f'Score: {score:<.4f}  Scores: {scores}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a295ac36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T17:44:41.601871Z",
     "iopub.status.busy": "2022-11-29T17:44:41.600997Z",
     "iopub.status.idle": "2022-11-29T17:44:41.634333Z",
     "shell.execute_reply": "2022-11-29T17:44:41.633481Z"
    },
    "papermill": {
     "duration": 0.045821,
     "end_time": "2022-11-29T17:44:41.636328",
     "exception": false,
     "start_time": "2022-11-29T17:44:41.590507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(CFG.test_file)\n",
    "submission = pd.read_csv(CFG.submission_file)\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"(\\n|\\r)+\",\" \",text.strip())\n",
    "    text = re.sub(r\"\\s+\",\" \",text)\n",
    "    return text\n",
    "test['text'] = test['full_text'].apply(clean_text)\n",
    "# sort by length to speed up inference\n",
    "test['tokenize_length'] = [len(TESTCFG.tokenizer(text)['input_ids']) for text in test['text'].values]\n",
    "test = test.sort_values('tokenize_length', ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b04adfd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T17:44:41.656744Z",
     "iopub.status.busy": "2022-11-29T17:44:41.655963Z",
     "iopub.status.idle": "2022-11-29T17:44:41.663234Z",
     "shell.execute_reply": "2022-11-29T17:44:41.662330Z"
    },
    "papermill": {
     "duration": 0.019401,
     "end_time": "2022-11-29T17:44:41.665235",
     "exception": false,
     "start_time": "2022-11-29T17:44:41.645834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_input(cfg, text):\n",
    "    inputs = cfg.tokenizer.encode_plus(\n",
    "        text, \n",
    "        return_tensors=None, \n",
    "        add_special_tokens=True, \n",
    "        #max_length=CFG.max_len,\n",
    "        #pad_to_max_length=True,\n",
    "        #truncation=True\n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['text'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg, self.texts[item])\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b6b427c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T17:44:41.684916Z",
     "iopub.status.busy": "2022-11-29T17:44:41.684664Z",
     "iopub.status.idle": "2022-11-29T17:44:41.690266Z",
     "shell.execute_reply": "2022-11-29T17:44:41.689352Z"
    },
    "papermill": {
     "duration": 0.017731,
     "end_time": "2022-11-29T17:44:41.692220",
     "exception": false,
     "start_time": "2022-11-29T17:44:41.674489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min = 1e-9)\n",
    "        mean_embeddings = sum_embeddings/sum_mask\n",
    "        return mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "292c826c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T17:44:41.711991Z",
     "iopub.status.busy": "2022-11-29T17:44:41.711453Z",
     "iopub.status.idle": "2022-11-29T17:44:41.724811Z",
     "shell.execute_reply": "2022-11-29T17:44:41.723977Z"
    },
    "papermill": {
     "duration": 0.025469,
     "end_time": "2022-11-29T17:44:41.726723",
     "exception": false,
     "start_time": "2022-11-29T17:44:41.701254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n",
    "            self.config.hidden_dropout = 0.\n",
    "            self.config.hidden_dropout_prob = 0.\n",
    "            self.config.attention_dropout = 0.\n",
    "            self.config.attention_probs_dropout_prob = 0.\n",
    "\n",
    "        else:\n",
    "            #self.config = torch.load(config_path)\n",
    "            self.config = AutoConfig.from_pretrained(config_path, output_hidden_states=True)\n",
    "\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel.from_config(self.config)\n",
    "        if self.cfg.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        if CFG.pooling == 'mean':\n",
    "            self.pool = MeanPooling()\n",
    "        elif CFG.pooling == 'max':\n",
    "            self.pool = MaxPooling()\n",
    "        elif CFG.pooling == 'min':\n",
    "            self.pool = MinPooling()\n",
    "        elif CFG.pooling == 'attention':\n",
    "#             print(\"using attention pooling\")\n",
    "            self.pool = AttentionPooling(self.config.hidden_size)\n",
    "        elif CFG.pooling == 'weightedlayer':\n",
    "            self.pool = WeightedLayerPooling(self.config.num_hidden_layers, layer_start = CFG.layer_start, layer_weights = None)        \n",
    "\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 6)\n",
    "        self._init_weights(self.fc)\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        feature = self.pool(last_hidden_states, inputs['attention_mask'])\n",
    "        return feature\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(feature)\n",
    "        return output                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e30a2b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T17:44:41.746599Z",
     "iopub.status.busy": "2022-11-29T17:44:41.745859Z",
     "iopub.status.idle": "2022-11-29T17:44:41.752257Z",
     "shell.execute_reply": "2022-11-29T17:44:41.751438Z"
    },
    "papermill": {
     "duration": 0.01835,
     "end_time": "2022-11-29T17:44:41.754243",
     "exception": false,
     "start_time": "2022-11-29T17:44:41.735893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# inference\n",
    "# ====================================================\n",
    "def inference_fn(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ed15a52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T17:44:41.774208Z",
     "iopub.status.busy": "2022-11-29T17:44:41.773481Z",
     "iopub.status.idle": "2022-11-29T17:45:34.189650Z",
     "shell.execute_reply": "2022-11-29T17:45:34.187245Z"
    },
    "papermill": {
     "duration": 52.430265,
     "end_time": "2022-11-29T17:45:34.193788",
     "exception": false,
     "start_time": "2022-11-29T17:44:41.763523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OUTPUT_DIR: ./20221122-054549-deberta-v3-base/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d405e9636994b659a6187a5a0f7e7be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243cfbc997e64cec8020de765c3f08bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b64c70319240dfb28ce4021acf6cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if CFG.test:\n",
    "    LOGGER = get_logger()\n",
    "    LOGGER.info(f'OUTPUT_DIR: {CFG.OUTPUT_DIR}')\n",
    "    test_dataset = TestDataset(TESTCFG, test)\n",
    "    test_loader = DataLoader(test_dataset,\n",
    "                             batch_size=TESTCFG.batch_size,\n",
    "                             shuffle=False,\n",
    "                             collate_fn=DataCollatorWithPadding(tokenizer=TESTCFG.tokenizer, padding='longest'),\n",
    "                             num_workers=TESTCFG.num_workers, pin_memory=True, drop_last=False)\n",
    "    predictions = []\n",
    "    for fold in TESTCFG.trn_fold:\n",
    "        model = CustomModel(TESTCFG, config_path=TESTCFG.config_path, pretrained=False)\n",
    "        state = torch.load(TESTCFG.path + \"/\"+f\"{TESTCFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n",
    "                           map_location=torch.device('cpu'))\n",
    "                           #map_location='cuda:0')\n",
    "        model.load_state_dict(state['model'])\n",
    "        prediction = inference_fn(test_loader, model, CFG.device)\n",
    "        predictions.append(prediction)\n",
    "        del model, state, prediction; gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    predictions = np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e817379c",
   "metadata": {
    "papermill": {
     "duration": 0.02325,
     "end_time": "2022-11-29T17:45:34.241617",
     "exception": false,
     "start_time": "2022-11-29T17:45:34.218367",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad7a910a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T17:45:34.282369Z",
     "iopub.status.busy": "2022-11-29T17:45:34.281970Z",
     "iopub.status.idle": "2022-11-29T17:45:34.315515Z",
     "shell.execute_reply": "2022-11-29T17:45:34.314350Z"
    },
    "papermill": {
     "duration": 0.05243,
     "end_time": "2022-11-29T17:45:34.318136",
     "exception": false,
     "start_time": "2022-11-29T17:45:34.265706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000C359D63E</td>\n",
       "      <td>2.800175</td>\n",
       "      <td>2.852544</td>\n",
       "      <td>3.201424</td>\n",
       "      <td>3.051314</td>\n",
       "      <td>2.756574</td>\n",
       "      <td>2.829673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000BAD50D026</td>\n",
       "      <td>2.664330</td>\n",
       "      <td>2.404221</td>\n",
       "      <td>2.711275</td>\n",
       "      <td>2.354718</td>\n",
       "      <td>2.032302</td>\n",
       "      <td>2.644668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00367BB2546B</td>\n",
       "      <td>3.597044</td>\n",
       "      <td>3.379902</td>\n",
       "      <td>3.470444</td>\n",
       "      <td>3.588651</td>\n",
       "      <td>3.385407</td>\n",
       "      <td>3.363934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n",
       "0  0000C359D63E  2.800175  2.852544    3.201424     3.051314  2.756574   \n",
       "1  000BAD50D026  2.664330  2.404221    2.711275     2.354718  2.032302   \n",
       "2  00367BB2546B  3.597044  3.379902    3.470444     3.588651  3.385407   \n",
       "\n",
       "   conventions  \n",
       "0     2.829673  \n",
       "1     2.644668  \n",
       "2     3.363934  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n"
     ]
    }
   ],
   "source": [
    "test[CFG.target_cols] = predictions.clip(1, 5)\n",
    "submission = submission.drop(columns=CFG.target_cols).merge(test[['text_id'] + CFG.target_cols], on='text_id', how='left')\n",
    "display(submission.head())\n",
    "submission[['text_id'] + CFG.target_cols].to_csv('submission.csv', index=False)\n",
    "print(\"completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e4fc03f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T17:45:34.340127Z",
     "iopub.status.busy": "2022-11-29T17:45:34.339847Z",
     "iopub.status.idle": "2022-11-29T17:45:34.344937Z",
     "shell.execute_reply": "2022-11-29T17:45:34.344023Z"
    },
    "papermill": {
     "duration": 0.018363,
     "end_time": "2022-11-29T17:45:34.346960",
     "exception": false,
     "start_time": "2022-11-29T17:45:34.328597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DONE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 72.010222,
   "end_time": "2022-11-29T17:45:37.077673",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-29T17:44:25.067451",
   "version": "2.3.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01b64c70319240dfb28ce4021acf6cbb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_766a0e3c504840ff9de9801b5d6d5136",
        "IPY_MODEL_6bf78286b2fd4e6498fe7e0603d94f46",
        "IPY_MODEL_16679c8f537a476ca56e2c584ee332a1"
       ],
       "layout": "IPY_MODEL_07818cbd8cdc4d9ca8b151cb6b644715"
      }
     },
     "02abb2a829cd4f0c9c93d6292d2f8605": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "03e3ea3c6fd2423992a7b2811cb2269d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "07818cbd8cdc4d9ca8b151cb6b644715": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "14c789671d2740fb813e52841c0f8b47": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "16679c8f537a476ca56e2c584ee332a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_94061cfaa96f45ecb8c93d85b038e7a5",
       "placeholder": "​",
       "style": "IPY_MODEL_352d9cb77aad4d1fb135fb5007d9048c",
       "value": " 1/1 [00:00&lt;00:00,  1.62it/s]"
      }
     },
     "1d405e9636994b659a6187a5a0f7e7be": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a23424b82a184acab6ff845caedc1266",
        "IPY_MODEL_e4a1cd59b6374a8991e678ad174db0d5",
        "IPY_MODEL_ed365a48ec2a4b9b9cf591d10717e74e"
       ],
       "layout": "IPY_MODEL_03e3ea3c6fd2423992a7b2811cb2269d"
      }
     },
     "232e623199c24dfdac6cee19252fa5f2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "243cfbc997e64cec8020de765c3f08bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_25fce9ead6694e0e9d82033f22366c11",
        "IPY_MODEL_6aecc72b67514738a36621424bfda41b",
        "IPY_MODEL_ababfbf2e0b4430296fabe76e5999ac0"
       ],
       "layout": "IPY_MODEL_9544fa79155c4b15babd973d36f2714e"
      }
     },
     "25fce9ead6694e0e9d82033f22366c11": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9d99ec283f6f4179b995ec182c596ef0",
       "placeholder": "​",
       "style": "IPY_MODEL_2985b37557634a6a80f8a609e9589896",
       "value": "100%"
      }
     },
     "2985b37557634a6a80f8a609e9589896": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "352d9cb77aad4d1fb135fb5007d9048c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "46b6bd75431f4adba42b29e74e875e67": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "54c28c3d6406477d9e1bc1c8d30eb9ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6aecc72b67514738a36621424bfda41b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_46b6bd75431f4adba42b29e74e875e67",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f5bf4a44d38347b4b38a7ea11d478455",
       "value": 1.0
      }
     },
     "6bf78286b2fd4e6498fe7e0603d94f46": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b20b370f1aad4676877cc34420992a44",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8381bbfbb355490598c329ab6c13c58d",
       "value": 1.0
      }
     },
     "766a0e3c504840ff9de9801b5d6d5136": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_bffb643ded3a43adafbc50ed58ed3b84",
       "placeholder": "​",
       "style": "IPY_MODEL_8009dff07a384825b276fb90d39024d0",
       "value": "100%"
      }
     },
     "8009dff07a384825b276fb90d39024d0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8381bbfbb355490598c329ab6c13c58d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "863efd0d8a49404fa52d635cc35ce244": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "94061cfaa96f45ecb8c93d85b038e7a5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9544fa79155c4b15babd973d36f2714e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9d99ec283f6f4179b995ec182c596ef0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a23424b82a184acab6ff845caedc1266": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_02abb2a829cd4f0c9c93d6292d2f8605",
       "placeholder": "​",
       "style": "IPY_MODEL_cb68fd9c59b547c7b8ac61605b78deca",
       "value": "100%"
      }
     },
     "ababfbf2e0b4430296fabe76e5999ac0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_14c789671d2740fb813e52841c0f8b47",
       "placeholder": "​",
       "style": "IPY_MODEL_54c28c3d6406477d9e1bc1c8d30eb9ef",
       "value": " 1/1 [00:00&lt;00:00,  1.62it/s]"
      }
     },
     "b20b370f1aad4676877cc34420992a44": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bffb643ded3a43adafbc50ed58ed3b84": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c38b3d493ec44259be6962eaa35fb2bf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cb68fd9c59b547c7b8ac61605b78deca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e4a1cd59b6374a8991e678ad174db0d5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_232e623199c24dfdac6cee19252fa5f2",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_863efd0d8a49404fa52d635cc35ce244",
       "value": 1.0
      }
     },
     "eca420c05b064fa885bcd9d445f55403": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ed365a48ec2a4b9b9cf591d10717e74e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c38b3d493ec44259be6962eaa35fb2bf",
       "placeholder": "​",
       "style": "IPY_MODEL_eca420c05b064fa885bcd9d445f55403",
       "value": " 1/1 [00:01&lt;00:00,  1.83s/it]"
      }
     },
     "f5bf4a44d38347b4b38a7ea11d478455": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
